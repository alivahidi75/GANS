{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGQgCExS7e3Rlab52QkVpZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alivahidi75/GANS/blob/main/TripleGAN_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mt\n",
        "import matplotlib.pyplot as pt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics         import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras as ks\n",
        "from keras import layers as lys\n",
        "from keras import models as mls\n",
        "from keras import initializers\n",
        "from keras.utils import to_categorical as ct\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, initializers\n",
        "import io\n",
        "import gdown\n",
        "import scipy.io"
      ],
      "metadata": {
        "id": "XSdcsjuUhKH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVrGQ0qQ-6oA"
      },
      "outputs": [],
      "source": [
        "class TripleGAN(tf.keras.Model):\n",
        "    def __init__(self, generator, discriminator, classifier, latent_dim):\n",
        "        super(TripleGAN, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.classifier = classifier\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def compile(self, g_optimizer, d_optimizer, c_optimizer, g_loss_fn, d_loss_fn, c_loss_fn):\n",
        "        super(TripleGAN, self).compile()\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.c_optimizer = c_optimizer\n",
        "        self.g_loss_fn = g_loss_fn\n",
        "        self.d_loss_fn = d_loss_fn\n",
        "        self.c_loss_fn = c_loss_fn\n",
        "\n",
        "    def train_step(self, sup_images, labels, unsup_images):\n",
        "        #real_images, real_labels = data  # Assuming data is a tuple of (images, labels)\n",
        "        real_images = tf.concat([sup_images, unsup_images], axis=0)\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        # Generate fake images\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "\n",
        "        # Labels for real and fake images\n",
        "        real_labels_disc = tf.ones((batch_size, 1))\n",
        "        fake_labels_disc = tf.zeros((batch_size, 1))\n",
        "\n",
        "        # Train the discriminator\n",
        "        with tf.GradientTape() as d_tape:\n",
        "            real_predictions = self.discriminator(real_images)\n",
        "            fake_predictions = self.discriminator(generated_images)\n",
        "            d_loss = self.d_loss_fn(real_labels_disc, real_predictions) + self.d_loss_fn(fake_labels_disc, fake_predictions)\n",
        "\n",
        "        d_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_weights))\n",
        "\n",
        "        # Train the generator\n",
        "        with tf.GradientTape() as g_tape:\n",
        "            generated_images = self.generator(random_latent_vectors)\n",
        "            fake_predictions = self.discriminator(generated_images)\n",
        "            g_loss = self.g_loss_fn(real_labels_disc, fake_predictions)\n",
        "\n",
        "        g_gradients = g_tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_weights))\n",
        "\n",
        "        # Train the classifier\n",
        "        with tf.GradientTape() as c_tape:\n",
        "            # Classify real labeled data\n",
        "            real_classifications = self.classifier(sup_images)\n",
        "            c_loss = self.c_loss_fn(labels, real_classifications)\n",
        "\n",
        "        c_gradients = c_tape.gradient(c_loss, self.classifier.trainable_weights)\n",
        "        self.c_optimizer.apply_gradients(zip(c_gradients, self.classifier.trainable_weights))\n",
        "\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"c_loss\": c_loss}\n",
        "\n",
        "    def fit(self, labeled_dataset, unlabeled_dataset, epochs, callbacks=None):\n",
        "      combined_dataset = tf.data.Dataset.zip((labeled_dataset, unlabeled_dataset))\n",
        "      for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Iterate over the combined dataset\n",
        "        for step, ((sup_images, labels), unsup_images) in enumerate(combined_dataset):\n",
        "            # Perform a single training step\n",
        "            losses = self.train_step(sup_images, labels, unsup_images)\n",
        "\n",
        "            # Log losses\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Step {step}: d_loss={losses['d_loss']:.4f}, g_loss={losses['g_loss']:.4f}, c_loss={losses['c_loss']:.4f}\")\n",
        "\n",
        "        # Run callbacks at the end of each epoch\n",
        "        if callbacks:\n",
        "            for callback in callbacks:\n",
        "                callback.on_epoch_end(epoch, logs=losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 7\n",
        "\n",
        "dr = ks.Sequential(name=\"Discriminator\")\n",
        "dr.add(lys.Input(shape=(latent_dim,)))\n",
        "dr.add(lys.Dense(units=180, activation='sigmoid',kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "dr.add(lys.LeakyReLU(alpha=0.4))\n",
        "dr.add(lys.Dense(units=140, activation='sigmoid',  kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "dr.add(lys.LeakyReLU(alpha=0.4))\n",
        "dr.add(lys.Dense(units=120, activation='sigmoid',  kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "dr.add(lys.LeakyReLU(alpha=0.4))\n",
        "dr.add(lys.Dropout(0.4))\n",
        "dr.add(lys.Dense(1))\n",
        "dr.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "7wkZCKMYhiFD",
        "outputId": "e23ddf6a-07ca-4839-cf0c-f90edb258f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Discriminator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Discriminator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_46 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)                 │           \u001b[38;5;34m1,440\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_12 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_47 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m)                 │          \u001b[38;5;34m25,340\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_13 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m140\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)                 │          \u001b[38;5;34m16,920\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_14 (\u001b[38;5;33mLeakyReLU\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m121\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,440</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_47 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">25,340</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">140</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,920</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ leaky_re_lu_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">121</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,821\u001b[0m (171.18 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,821</span> (171.18 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,821\u001b[0m (171.18 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,821</span> (171.18 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gr = ks.Sequential(name = \"Generator\")\n",
        "gr.add(lys.Input(shape=(latent_dim,)))\n",
        "gr.add(lys.Dense(180, activation='tanh', kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "gr.add(lys.Dense(100, activation='tanh',kernel_initializer=tf.keras.initializers.GlorotNormal(),bias_initializer=initializers.Zeros()))\n",
        "gr.add(lys.Dense(7, activation='relu'))\n",
        "gr.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "03XXYLQ1hiL4",
        "outputId": "ead01117-79b2-46f7-93f8-9a788ef5811c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Generator\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Generator\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_50 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m180\u001b[0m)                 │           \u001b[38;5;34m1,440\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_51 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)                 │          \u001b[38;5;34m18,100\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_52 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │             \u001b[38;5;34m707\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,440</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,100</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">707</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,247\u001b[0m (79.09 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,247</span> (79.09 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,247\u001b[0m (79.09 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,247</span> (79.09 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cr = ks.Sequential(name = \"Classifier\")\n",
        "cr.add(lys.Input(shape = (7,)))\n",
        "cr.add(lys.Dense(256, activation = 'relu'))\n",
        "cr.add(lys.Dense(128, activation = 'relu'))\n",
        "cr.add(lys.Dense(10, activation = 'softmax'))\n",
        "cr.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "I6TEDEvqhsOw",
        "outputId": "71bb361a-bb42-4c6b-bf9a-1d8fa5c06bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"Classifier\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Classifier\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_53 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │           \u001b[38;5;34m2,048\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_54 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_55 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_55 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m36,234\u001b[0m (141.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,234</span> (141.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m36,234\u001b[0m (141.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">36,234</span> (141.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = \"1Z_mEDB60-BnoKCEbiYqxKchk4NSgLcKW\"\n",
        "output_name = \"downloaded_file.xlsx\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_name, quiet=False)\n",
        "data = pd.read_excel(output_name)\n",
        "data = data.to_numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwLzjaM4jY-P",
        "outputId": "7d4b4cc7-7634-4955-8200-6f514c0a3a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Z_mEDB60-BnoKCEbiYqxKchk4NSgLcKW\n",
            "To: /content/downloaded_file.xlsx\n",
            "100%|██████████| 396k/396k [00:00<00:00, 65.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "input = data[:, 1:8]\n",
        "output = data[:, 0]\n",
        "output = output - 1\n",
        "\n",
        "f = input.shape[1]\n",
        "pca = PCA(n_components=7)\n",
        "pca.fit(input)\n",
        "input= pca.transform(input)\n",
        "minmax = MinMaxScaler()\n",
        "dataset =  minmax.fit_transform(input).astype(np.float32)\n",
        "#dataset = normalizeData(input).astype(np.float32)  # Convert to float32\n",
        "dataset = tf.data.Dataset.from_tensor_slices(dataset).batch(512)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "V5p2MnZmjZrx",
        "outputId": "35f6e86f-eeea-4993-ea52-e1a6d5487ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ninput = data[:, 1:8]\\noutput = data[:, 0]\\noutput = output - 1\\n\\nf = input.shape[1]\\npca = PCA(n_components=7)\\npca.fit(input)\\ninput= pca.transform(input)\\nminmax = MinMaxScaler()\\ndataset =  minmax.fit_transform(input).astype(np.float32)\\n#dataset = normalizeData(input).astype(np.float32)  # Convert to float32\\ndataset = tf.data.Dataset.from_tensor_slices(dataset).batch(512)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "input = data[:, 1:8]\n",
        "output = data[:, 0]\n",
        "output = output - 1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=0.1, random_state=42)\n",
        "\n",
        "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=7)\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "X_labeled_pca = pca.transform(X_labeled)\n",
        "X_unlabeled_pca = pca.transform(X_unlabeled)\n",
        "\n",
        "minmax = MinMaxScaler()\n",
        "minmax.fit(X_train_pca)\n",
        "\n",
        "X_train_scaled = minmax.transform(X_train_pca).astype(np.float32)\n",
        "X_test_scaled = minmax.transform(X_test_pca).astype(np.float32)\n",
        "X_labeled_scaled = minmax.transform(X_labeled_pca).astype(np.float32)\n",
        "X_unlabeled_scaled = minmax.transform(X_unlabeled_pca).astype(np.float32)\n",
        "\n",
        "\n",
        "num_classes = len(np.unique(y_labeled))\n",
        "y_labeled_onehot = tf.keras.utils.to_categorical(y_labeled, num_classes=num_classes)\n",
        "labels = y_labeled_onehot\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "labeled_dataset = tf.data.Dataset.from_tensor_slices((X_labeled_scaled, y_labeled_onehot))\n",
        "labeled_dataset = labeled_dataset.shuffle(buffer_size=len(X_labeled_scaled)).batch(batch_size)\n",
        "\n",
        "\n",
        "unlabeled_dataset = tf.data.Dataset.from_tensor_slices(X_unlabeled_scaled)\n",
        "unlabeled_dataset = unlabeled_dataset.shuffle(buffer_size=len(X_unlabeled_scaled)).batch(batch_size)\n",
        "\n",
        "num_classes = len(np.unique(y_test))\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test_onehot))\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "sc_sYLCSkrOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triple_gan = TripleGAN(gr, dr, cr, latent_dim)\n",
        "triple_gan.compile(\n",
        "    g_optimizer=tf.keras.optimizers.Adam(learning_rate= ks.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001,decay_steps=10000,decay_rate=0.5),beta_1=0.5),\n",
        "    d_optimizer=tf.keras.optimizers.Adam(learning_rate= ks.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001,decay_steps=10000,decay_rate=0.5),beta_1=0.5),\n",
        "    c_optimizer=tf.keras.optimizers.Adam(learning_rate= ks.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.0001,decay_steps=10000,decay_rate=0.5),beta_1=0.5),\n",
        "    g_loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
        "    d_loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
        "    c_loss_fn=tf.keras.losses.CategoricalCrossentropy())"
      ],
      "metadata": {
        "id": "qSMx6FnkhiYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = triple_gan.fit(labeled_dataset, unlabeled_dataset, epochs=2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy6jAhnQlFHy",
        "outputId": "cf1e58f7-a454-44aa-d26a-5b31feea77ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2987\n",
            "Epoch 2/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2969\n",
            "Epoch 3/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2952\n",
            "Epoch 4/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2935\n",
            "Epoch 5/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2919\n",
            "Epoch 6/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2902\n",
            "Epoch 7/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2886\n",
            "Epoch 8/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2870\n",
            "Epoch 9/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2854\n",
            "Epoch 10/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2839\n",
            "Epoch 11/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2823\n",
            "Epoch 12/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2807\n",
            "Epoch 13/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2791\n",
            "Epoch 14/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2774\n",
            "Epoch 15/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2757\n",
            "Epoch 16/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2740\n",
            "Epoch 17/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2723\n",
            "Epoch 18/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2707\n",
            "Epoch 19/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2691\n",
            "Epoch 20/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2675\n",
            "Epoch 21/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2659\n",
            "Epoch 22/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2644\n",
            "Epoch 23/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2628\n",
            "Epoch 24/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2612\n",
            "Epoch 25/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2597\n",
            "Epoch 26/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2581\n",
            "Epoch 27/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2566\n",
            "Epoch 28/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2551\n",
            "Epoch 29/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2536\n",
            "Epoch 30/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2521\n",
            "Epoch 31/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2507\n",
            "Epoch 32/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2492\n",
            "Epoch 33/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2478\n",
            "Epoch 34/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2464\n",
            "Epoch 35/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2450\n",
            "Epoch 36/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2435\n",
            "Epoch 37/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2421\n",
            "Epoch 38/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2407\n",
            "Epoch 39/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2393\n",
            "Epoch 40/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2379\n",
            "Epoch 41/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2365\n",
            "Epoch 42/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2351\n",
            "Epoch 43/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2336\n",
            "Epoch 44/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2322\n",
            "Epoch 45/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2308\n",
            "Epoch 46/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2293\n",
            "Epoch 47/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2279\n",
            "Epoch 48/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2264\n",
            "Epoch 49/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2250\n",
            "Epoch 50/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2236\n",
            "Epoch 51/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2222\n",
            "Epoch 52/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2208\n",
            "Epoch 53/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2194\n",
            "Epoch 54/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2180\n",
            "Epoch 55/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2166\n",
            "Epoch 56/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2152\n",
            "Epoch 57/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2138\n",
            "Epoch 58/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2124\n",
            "Epoch 59/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2110\n",
            "Epoch 60/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2096\n",
            "Epoch 61/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2082\n",
            "Epoch 62/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2068\n",
            "Epoch 63/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2054\n",
            "Epoch 64/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2040\n",
            "Epoch 65/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2026\n",
            "Epoch 66/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.2011\n",
            "Epoch 67/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1997\n",
            "Epoch 68/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1983\n",
            "Epoch 69/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1968\n",
            "Epoch 70/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1954\n",
            "Epoch 71/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1939\n",
            "Epoch 72/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1925\n",
            "Epoch 73/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1910\n",
            "Epoch 74/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1895\n",
            "Epoch 75/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1881\n",
            "Epoch 76/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1866\n",
            "Epoch 77/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1851\n",
            "Epoch 78/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1836\n",
            "Epoch 79/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1821\n",
            "Epoch 80/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1806\n",
            "Epoch 81/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1791\n",
            "Epoch 82/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1775\n",
            "Epoch 83/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1760\n",
            "Epoch 84/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1745\n",
            "Epoch 85/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1729\n",
            "Epoch 86/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1714\n",
            "Epoch 87/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1698\n",
            "Epoch 88/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1682\n",
            "Epoch 89/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1666\n",
            "Epoch 90/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1650\n",
            "Epoch 91/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1634\n",
            "Epoch 92/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1618\n",
            "Epoch 93/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1602\n",
            "Epoch 94/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1586\n",
            "Epoch 95/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1569\n",
            "Epoch 96/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1553\n",
            "Epoch 97/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1536\n",
            "Epoch 98/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1520\n",
            "Epoch 99/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1503\n",
            "Epoch 100/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1487\n",
            "Epoch 101/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1470\n",
            "Epoch 102/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1453\n",
            "Epoch 103/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1436\n",
            "Epoch 104/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1420\n",
            "Epoch 105/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1403\n",
            "Epoch 106/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1386\n",
            "Epoch 107/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1369\n",
            "Epoch 108/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1351\n",
            "Epoch 109/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1334\n",
            "Epoch 110/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1317\n",
            "Epoch 111/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1300\n",
            "Epoch 112/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1283\n",
            "Epoch 113/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1265\n",
            "Epoch 114/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1248\n",
            "Epoch 115/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1230\n",
            "Epoch 116/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1212\n",
            "Epoch 117/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1195\n",
            "Epoch 118/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1177\n",
            "Epoch 119/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1159\n",
            "Epoch 120/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1141\n",
            "Epoch 121/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1123\n",
            "Epoch 122/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1105\n",
            "Epoch 123/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1086\n",
            "Epoch 124/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1068\n",
            "Epoch 125/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1049\n",
            "Epoch 126/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1031\n",
            "Epoch 127/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.1012\n",
            "Epoch 128/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0993\n",
            "Epoch 129/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0974\n",
            "Epoch 130/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0955\n",
            "Epoch 131/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0936\n",
            "Epoch 132/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0916\n",
            "Epoch 133/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0897\n",
            "Epoch 134/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0877\n",
            "Epoch 135/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0858\n",
            "Epoch 136/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0838\n",
            "Epoch 137/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0818\n",
            "Epoch 138/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0798\n",
            "Epoch 139/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0778\n",
            "Epoch 140/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0757\n",
            "Epoch 141/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0737\n",
            "Epoch 142/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0716\n",
            "Epoch 143/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0696\n",
            "Epoch 144/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0675\n",
            "Epoch 145/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0654\n",
            "Epoch 146/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0634\n",
            "Epoch 147/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0613\n",
            "Epoch 148/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0591\n",
            "Epoch 149/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0570\n",
            "Epoch 150/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0549\n",
            "Epoch 151/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0527\n",
            "Epoch 152/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0506\n",
            "Epoch 153/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0484\n",
            "Epoch 154/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0462\n",
            "Epoch 155/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0440\n",
            "Epoch 156/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0417\n",
            "Epoch 157/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0395\n",
            "Epoch 158/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0372\n",
            "Epoch 159/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0348\n",
            "Epoch 160/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0325\n",
            "Epoch 161/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0301\n",
            "Epoch 162/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0277\n",
            "Epoch 163/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0253\n",
            "Epoch 164/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0230\n",
            "Epoch 165/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0206\n",
            "Epoch 166/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0182\n",
            "Epoch 167/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0159\n",
            "Epoch 168/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0135\n",
            "Epoch 169/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0111\n",
            "Epoch 170/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0086\n",
            "Epoch 171/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0060\n",
            "Epoch 172/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0034\n",
            "Epoch 173/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=2.0008\n",
            "Epoch 174/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9982\n",
            "Epoch 175/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9957\n",
            "Epoch 176/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9931\n",
            "Epoch 177/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9905\n",
            "Epoch 178/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9879\n",
            "Epoch 179/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9853\n",
            "Epoch 180/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9827\n",
            "Epoch 181/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9802\n",
            "Epoch 182/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9776\n",
            "Epoch 183/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9750\n",
            "Epoch 184/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9725\n",
            "Epoch 185/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9699\n",
            "Epoch 186/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9673\n",
            "Epoch 187/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9648\n",
            "Epoch 188/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9622\n",
            "Epoch 189/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9596\n",
            "Epoch 190/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9570\n",
            "Epoch 191/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9544\n",
            "Epoch 192/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9518\n",
            "Epoch 193/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9492\n",
            "Epoch 194/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9466\n",
            "Epoch 195/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9440\n",
            "Epoch 196/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9413\n",
            "Epoch 197/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9387\n",
            "Epoch 198/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9360\n",
            "Epoch 199/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9334\n",
            "Epoch 200/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9307\n",
            "Epoch 201/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9280\n",
            "Epoch 202/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9253\n",
            "Epoch 203/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9226\n",
            "Epoch 204/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9200\n",
            "Epoch 205/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9173\n",
            "Epoch 206/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9146\n",
            "Epoch 207/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9118\n",
            "Epoch 208/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9091\n",
            "Epoch 209/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9064\n",
            "Epoch 210/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9036\n",
            "Epoch 211/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.9009\n",
            "Epoch 212/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8981\n",
            "Epoch 213/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8953\n",
            "Epoch 214/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8926\n",
            "Epoch 215/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8898\n",
            "Epoch 216/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8870\n",
            "Epoch 217/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8842\n",
            "Epoch 218/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8815\n",
            "Epoch 219/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8787\n",
            "Epoch 220/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8758\n",
            "Epoch 221/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8730\n",
            "Epoch 222/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8702\n",
            "Epoch 223/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8674\n",
            "Epoch 224/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8645\n",
            "Epoch 225/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8616\n",
            "Epoch 226/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8588\n",
            "Epoch 227/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8559\n",
            "Epoch 228/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8530\n",
            "Epoch 229/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8501\n",
            "Epoch 230/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8473\n",
            "Epoch 231/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8444\n",
            "Epoch 232/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8415\n",
            "Epoch 233/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8386\n",
            "Epoch 234/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8358\n",
            "Epoch 235/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8329\n",
            "Epoch 236/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8300\n",
            "Epoch 237/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8271\n",
            "Epoch 238/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8242\n",
            "Epoch 239/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8213\n",
            "Epoch 240/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8183\n",
            "Epoch 241/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8154\n",
            "Epoch 242/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8124\n",
            "Epoch 243/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8095\n",
            "Epoch 244/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8066\n",
            "Epoch 245/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8037\n",
            "Epoch 246/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.8007\n",
            "Epoch 247/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7978\n",
            "Epoch 248/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7949\n",
            "Epoch 249/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7920\n",
            "Epoch 250/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7891\n",
            "Epoch 251/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7862\n",
            "Epoch 252/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7833\n",
            "Epoch 253/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7803\n",
            "Epoch 254/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7774\n",
            "Epoch 255/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7745\n",
            "Epoch 256/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7716\n",
            "Epoch 257/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7687\n",
            "Epoch 258/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7659\n",
            "Epoch 259/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7630\n",
            "Epoch 260/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7601\n",
            "Epoch 261/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7572\n",
            "Epoch 262/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7543\n",
            "Epoch 263/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7514\n",
            "Epoch 264/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7485\n",
            "Epoch 265/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7456\n",
            "Epoch 266/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7428\n",
            "Epoch 267/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7399\n",
            "Epoch 268/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7370\n",
            "Epoch 269/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7341\n",
            "Epoch 270/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7313\n",
            "Epoch 271/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7284\n",
            "Epoch 272/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7255\n",
            "Epoch 273/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7227\n",
            "Epoch 274/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7198\n",
            "Epoch 275/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7170\n",
            "Epoch 276/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7142\n",
            "Epoch 277/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7113\n",
            "Epoch 278/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7085\n",
            "Epoch 279/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7056\n",
            "Epoch 280/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7028\n",
            "Epoch 281/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.7000\n",
            "Epoch 282/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6972\n",
            "Epoch 283/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6944\n",
            "Epoch 284/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6915\n",
            "Epoch 285/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6887\n",
            "Epoch 286/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6859\n",
            "Epoch 287/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6831\n",
            "Epoch 288/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6803\n",
            "Epoch 289/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6774\n",
            "Epoch 290/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6746\n",
            "Epoch 291/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6718\n",
            "Epoch 292/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6689\n",
            "Epoch 293/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6661\n",
            "Epoch 294/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6633\n",
            "Epoch 295/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6605\n",
            "Epoch 296/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6576\n",
            "Epoch 297/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6548\n",
            "Epoch 298/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6519\n",
            "Epoch 299/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6491\n",
            "Epoch 300/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6463\n",
            "Epoch 301/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6435\n",
            "Epoch 302/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6407\n",
            "Epoch 303/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6379\n",
            "Epoch 304/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6351\n",
            "Epoch 305/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6323\n",
            "Epoch 306/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6295\n",
            "Epoch 307/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6267\n",
            "Epoch 308/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6239\n",
            "Epoch 309/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6212\n",
            "Epoch 310/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6184\n",
            "Epoch 311/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6156\n",
            "Epoch 312/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6129\n",
            "Epoch 313/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6102\n",
            "Epoch 314/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6074\n",
            "Epoch 315/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6047\n",
            "Epoch 316/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.6020\n",
            "Epoch 317/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5993\n",
            "Epoch 318/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5966\n",
            "Epoch 319/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5939\n",
            "Epoch 320/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5912\n",
            "Epoch 321/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5885\n",
            "Epoch 322/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5858\n",
            "Epoch 323/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5831\n",
            "Epoch 324/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5805\n",
            "Epoch 325/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5778\n",
            "Epoch 326/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5752\n",
            "Epoch 327/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5725\n",
            "Epoch 328/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5699\n",
            "Epoch 329/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5672\n",
            "Epoch 330/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5646\n",
            "Epoch 331/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5620\n",
            "Epoch 332/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5594\n",
            "Epoch 333/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5567\n",
            "Epoch 334/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5541\n",
            "Epoch 335/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5515\n",
            "Epoch 336/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5489\n",
            "Epoch 337/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5462\n",
            "Epoch 338/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5436\n",
            "Epoch 339/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5410\n",
            "Epoch 340/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5384\n",
            "Epoch 341/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5358\n",
            "Epoch 342/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5333\n",
            "Epoch 343/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5307\n",
            "Epoch 344/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5281\n",
            "Epoch 345/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5256\n",
            "Epoch 346/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5230\n",
            "Epoch 347/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5205\n",
            "Epoch 348/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5180\n",
            "Epoch 349/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5154\n",
            "Epoch 350/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5129\n",
            "Epoch 351/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5104\n",
            "Epoch 352/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5079\n",
            "Epoch 353/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5053\n",
            "Epoch 354/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5028\n",
            "Epoch 355/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.5002\n",
            "Epoch 356/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4977\n",
            "Epoch 357/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4952\n",
            "Epoch 358/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4927\n",
            "Epoch 359/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4902\n",
            "Epoch 360/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4877\n",
            "Epoch 361/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4852\n",
            "Epoch 362/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4827\n",
            "Epoch 363/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4802\n",
            "Epoch 364/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4777\n",
            "Epoch 365/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4752\n",
            "Epoch 366/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4728\n",
            "Epoch 367/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4703\n",
            "Epoch 368/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4678\n",
            "Epoch 369/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4654\n",
            "Epoch 370/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4629\n",
            "Epoch 371/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4605\n",
            "Epoch 372/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4580\n",
            "Epoch 373/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4556\n",
            "Epoch 374/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4531\n",
            "Epoch 375/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4507\n",
            "Epoch 376/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4483\n",
            "Epoch 377/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4459\n",
            "Epoch 378/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4436\n",
            "Epoch 379/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4412\n",
            "Epoch 380/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4388\n",
            "Epoch 381/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4365\n",
            "Epoch 382/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4341\n",
            "Epoch 383/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4318\n",
            "Epoch 384/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4294\n",
            "Epoch 385/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4271\n",
            "Epoch 386/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4248\n",
            "Epoch 387/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4225\n",
            "Epoch 388/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4202\n",
            "Epoch 389/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4179\n",
            "Epoch 390/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4156\n",
            "Epoch 391/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4133\n",
            "Epoch 392/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4110\n",
            "Epoch 393/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4087\n",
            "Epoch 394/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4065\n",
            "Epoch 395/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4042\n",
            "Epoch 396/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.4020\n",
            "Epoch 397/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3997\n",
            "Epoch 398/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3975\n",
            "Epoch 399/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3952\n",
            "Epoch 400/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3930\n",
            "Epoch 401/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3908\n",
            "Epoch 402/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3886\n",
            "Epoch 403/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3863\n",
            "Epoch 404/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3841\n",
            "Epoch 405/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3819\n",
            "Epoch 406/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3797\n",
            "Epoch 407/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3775\n",
            "Epoch 408/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3753\n",
            "Epoch 409/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3732\n",
            "Epoch 410/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3710\n",
            "Epoch 411/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3688\n",
            "Epoch 412/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3666\n",
            "Epoch 413/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3645\n",
            "Epoch 414/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3623\n",
            "Epoch 415/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3601\n",
            "Epoch 416/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3580\n",
            "Epoch 417/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3558\n",
            "Epoch 418/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3537\n",
            "Epoch 419/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3515\n",
            "Epoch 420/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3494\n",
            "Epoch 421/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3472\n",
            "Epoch 422/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3451\n",
            "Epoch 423/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3430\n",
            "Epoch 424/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3409\n",
            "Epoch 425/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3387\n",
            "Epoch 426/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3366\n",
            "Epoch 427/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3345\n",
            "Epoch 428/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3324\n",
            "Epoch 429/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3303\n",
            "Epoch 430/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3282\n",
            "Epoch 431/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3261\n",
            "Epoch 432/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3240\n",
            "Epoch 433/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3220\n",
            "Epoch 434/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3199\n",
            "Epoch 435/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3178\n",
            "Epoch 436/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3157\n",
            "Epoch 437/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3137\n",
            "Epoch 438/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3116\n",
            "Epoch 439/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3096\n",
            "Epoch 440/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3075\n",
            "Epoch 441/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3055\n",
            "Epoch 442/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3035\n",
            "Epoch 443/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.3014\n",
            "Epoch 444/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2994\n",
            "Epoch 445/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2974\n",
            "Epoch 446/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2954\n",
            "Epoch 447/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2934\n",
            "Epoch 448/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2914\n",
            "Epoch 449/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2894\n",
            "Epoch 450/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2874\n",
            "Epoch 451/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2854\n",
            "Epoch 452/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2835\n",
            "Epoch 453/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2815\n",
            "Epoch 454/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2795\n",
            "Epoch 455/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2775\n",
            "Epoch 456/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2756\n",
            "Epoch 457/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2736\n",
            "Epoch 458/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2717\n",
            "Epoch 459/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2697\n",
            "Epoch 460/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2677\n",
            "Epoch 461/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2657\n",
            "Epoch 462/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2638\n",
            "Epoch 463/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2618\n",
            "Epoch 464/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2599\n",
            "Epoch 465/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2579\n",
            "Epoch 466/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2560\n",
            "Epoch 467/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2541\n",
            "Epoch 468/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2521\n",
            "Epoch 469/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2502\n",
            "Epoch 470/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2483\n",
            "Epoch 471/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2464\n",
            "Epoch 472/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2444\n",
            "Epoch 473/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2425\n",
            "Epoch 474/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2406\n",
            "Epoch 475/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2387\n",
            "Epoch 476/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2368\n",
            "Epoch 477/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2350\n",
            "Epoch 478/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2331\n",
            "Epoch 479/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2312\n",
            "Epoch 480/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2293\n",
            "Epoch 481/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2275\n",
            "Epoch 482/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2256\n",
            "Epoch 483/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2237\n",
            "Epoch 484/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2219\n",
            "Epoch 485/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2200\n",
            "Epoch 486/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2182\n",
            "Epoch 487/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2164\n",
            "Epoch 488/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2145\n",
            "Epoch 489/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2127\n",
            "Epoch 490/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2109\n",
            "Epoch 491/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2090\n",
            "Epoch 492/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2072\n",
            "Epoch 493/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2054\n",
            "Epoch 494/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2036\n",
            "Epoch 495/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2018\n",
            "Epoch 496/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.2000\n",
            "Epoch 497/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1982\n",
            "Epoch 498/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1964\n",
            "Epoch 499/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1946\n",
            "Epoch 500/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1927\n",
            "Epoch 501/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1909\n",
            "Epoch 502/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1891\n",
            "Epoch 503/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1873\n",
            "Epoch 504/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1855\n",
            "Epoch 505/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1837\n",
            "Epoch 506/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1819\n",
            "Epoch 507/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1801\n",
            "Epoch 508/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1783\n",
            "Epoch 509/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1765\n",
            "Epoch 510/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1748\n",
            "Epoch 511/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1730\n",
            "Epoch 512/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1712\n",
            "Epoch 513/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1694\n",
            "Epoch 514/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1676\n",
            "Epoch 515/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1659\n",
            "Epoch 516/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1641\n",
            "Epoch 517/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1624\n",
            "Epoch 518/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1607\n",
            "Epoch 519/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1589\n",
            "Epoch 520/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1572\n",
            "Epoch 521/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1555\n",
            "Epoch 522/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1538\n",
            "Epoch 523/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1521\n",
            "Epoch 524/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1504\n",
            "Epoch 525/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1487\n",
            "Epoch 526/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1470\n",
            "Epoch 527/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1453\n",
            "Epoch 528/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1436\n",
            "Epoch 529/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1419\n",
            "Epoch 530/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1403\n",
            "Epoch 531/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1386\n",
            "Epoch 532/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1369\n",
            "Epoch 533/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1353\n",
            "Epoch 534/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1336\n",
            "Epoch 535/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1320\n",
            "Epoch 536/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1303\n",
            "Epoch 537/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1287\n",
            "Epoch 538/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1270\n",
            "Epoch 539/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1254\n",
            "Epoch 540/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1238\n",
            "Epoch 541/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1222\n",
            "Epoch 542/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1205\n",
            "Epoch 543/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1189\n",
            "Epoch 544/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1173\n",
            "Epoch 545/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1157\n",
            "Epoch 546/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1141\n",
            "Epoch 547/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1125\n",
            "Epoch 548/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1109\n",
            "Epoch 549/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1093\n",
            "Epoch 550/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1077\n",
            "Epoch 551/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1062\n",
            "Epoch 552/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1046\n",
            "Epoch 553/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1030\n",
            "Epoch 554/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.1014\n",
            "Epoch 555/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0999\n",
            "Epoch 556/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0983\n",
            "Epoch 557/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0968\n",
            "Epoch 558/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0952\n",
            "Epoch 559/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0937\n",
            "Epoch 560/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0921\n",
            "Epoch 561/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0906\n",
            "Epoch 562/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0890\n",
            "Epoch 563/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0875\n",
            "Epoch 564/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0860\n",
            "Epoch 565/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0845\n",
            "Epoch 566/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0829\n",
            "Epoch 567/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0814\n",
            "Epoch 568/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0798\n",
            "Epoch 569/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0783\n",
            "Epoch 570/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0768\n",
            "Epoch 571/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0752\n",
            "Epoch 572/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0737\n",
            "Epoch 573/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0722\n",
            "Epoch 574/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0707\n",
            "Epoch 575/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0692\n",
            "Epoch 576/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0678\n",
            "Epoch 577/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0663\n",
            "Epoch 578/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0648\n",
            "Epoch 579/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0633\n",
            "Epoch 580/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0618\n",
            "Epoch 581/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0604\n",
            "Epoch 582/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0589\n",
            "Epoch 583/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0574\n",
            "Epoch 584/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0560\n",
            "Epoch 585/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0545\n",
            "Epoch 586/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0531\n",
            "Epoch 587/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0516\n",
            "Epoch 588/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0502\n",
            "Epoch 589/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0487\n",
            "Epoch 590/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0473\n",
            "Epoch 591/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0459\n",
            "Epoch 592/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0444\n",
            "Epoch 593/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0430\n",
            "Epoch 594/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0416\n",
            "Epoch 595/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0402\n",
            "Epoch 596/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0388\n",
            "Epoch 597/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0374\n",
            "Epoch 598/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0359\n",
            "Epoch 599/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0345\n",
            "Epoch 600/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0331\n",
            "Epoch 601/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0317\n",
            "Epoch 602/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0303\n",
            "Epoch 603/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0289\n",
            "Epoch 604/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0276\n",
            "Epoch 605/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0262\n",
            "Epoch 606/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0248\n",
            "Epoch 607/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0234\n",
            "Epoch 608/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0220\n",
            "Epoch 609/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0207\n",
            "Epoch 610/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0193\n",
            "Epoch 611/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0179\n",
            "Epoch 612/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0166\n",
            "Epoch 613/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0152\n",
            "Epoch 614/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0138\n",
            "Epoch 615/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0125\n",
            "Epoch 616/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0111\n",
            "Epoch 617/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0098\n",
            "Epoch 618/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0084\n",
            "Epoch 619/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0071\n",
            "Epoch 620/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0058\n",
            "Epoch 621/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0044\n",
            "Epoch 622/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0031\n",
            "Epoch 623/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0018\n",
            "Epoch 624/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=1.0004\n",
            "Epoch 625/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9991\n",
            "Epoch 626/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9978\n",
            "Epoch 627/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9965\n",
            "Epoch 628/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9952\n",
            "Epoch 629/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9939\n",
            "Epoch 630/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9925\n",
            "Epoch 631/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9912\n",
            "Epoch 632/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9899\n",
            "Epoch 633/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9886\n",
            "Epoch 634/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9873\n",
            "Epoch 635/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9860\n",
            "Epoch 636/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9848\n",
            "Epoch 637/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9835\n",
            "Epoch 638/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9822\n",
            "Epoch 639/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9809\n",
            "Epoch 640/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9796\n",
            "Epoch 641/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9783\n",
            "Epoch 642/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9771\n",
            "Epoch 643/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9758\n",
            "Epoch 644/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9745\n",
            "Epoch 645/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9733\n",
            "Epoch 646/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9720\n",
            "Epoch 647/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9707\n",
            "Epoch 648/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9695\n",
            "Epoch 649/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9682\n",
            "Epoch 650/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9670\n",
            "Epoch 651/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9657\n",
            "Epoch 652/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9645\n",
            "Epoch 653/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9632\n",
            "Epoch 654/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9620\n",
            "Epoch 655/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9608\n",
            "Epoch 656/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9595\n",
            "Epoch 657/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9583\n",
            "Epoch 658/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9571\n",
            "Epoch 659/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9558\n",
            "Epoch 660/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9546\n",
            "Epoch 661/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9534\n",
            "Epoch 662/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9522\n",
            "Epoch 663/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9510\n",
            "Epoch 664/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9498\n",
            "Epoch 665/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9485\n",
            "Epoch 666/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9473\n",
            "Epoch 667/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9461\n",
            "Epoch 668/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9449\n",
            "Epoch 669/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9437\n",
            "Epoch 670/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9425\n",
            "Epoch 671/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9414\n",
            "Epoch 672/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9402\n",
            "Epoch 673/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9390\n",
            "Epoch 674/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9377\n",
            "Epoch 675/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9365\n",
            "Epoch 676/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9353\n",
            "Epoch 677/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9342\n",
            "Epoch 678/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9330\n",
            "Epoch 679/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9318\n",
            "Epoch 680/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9306\n",
            "Epoch 681/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9294\n",
            "Epoch 682/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9283\n",
            "Epoch 683/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9271\n",
            "Epoch 684/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9259\n",
            "Epoch 685/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9247\n",
            "Epoch 686/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9236\n",
            "Epoch 687/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9224\n",
            "Epoch 688/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9213\n",
            "Epoch 689/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9201\n",
            "Epoch 690/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9189\n",
            "Epoch 691/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9178\n",
            "Epoch 692/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9166\n",
            "Epoch 693/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9155\n",
            "Epoch 694/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9143\n",
            "Epoch 695/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9132\n",
            "Epoch 696/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9121\n",
            "Epoch 697/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9109\n",
            "Epoch 698/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9098\n",
            "Epoch 699/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9086\n",
            "Epoch 700/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9075\n",
            "Epoch 701/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9063\n",
            "Epoch 702/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9052\n",
            "Epoch 703/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9041\n",
            "Epoch 704/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9029\n",
            "Epoch 705/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9018\n",
            "Epoch 706/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.9007\n",
            "Epoch 707/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8996\n",
            "Epoch 708/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8985\n",
            "Epoch 709/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8973\n",
            "Epoch 710/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8962\n",
            "Epoch 711/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8951\n",
            "Epoch 712/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8940\n",
            "Epoch 713/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8929\n",
            "Epoch 714/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8918\n",
            "Epoch 715/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8907\n",
            "Epoch 716/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8896\n",
            "Epoch 717/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8885\n",
            "Epoch 718/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8874\n",
            "Epoch 719/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8863\n",
            "Epoch 720/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8852\n",
            "Epoch 721/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8842\n",
            "Epoch 722/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8831\n",
            "Epoch 723/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8820\n",
            "Epoch 724/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8809\n",
            "Epoch 725/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8798\n",
            "Epoch 726/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8788\n",
            "Epoch 727/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8777\n",
            "Epoch 728/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8766\n",
            "Epoch 729/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8756\n",
            "Epoch 730/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8745\n",
            "Epoch 731/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8734\n",
            "Epoch 732/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8724\n",
            "Epoch 733/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8713\n",
            "Epoch 734/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8703\n",
            "Epoch 735/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8692\n",
            "Epoch 736/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8682\n",
            "Epoch 737/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8671\n",
            "Epoch 738/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8661\n",
            "Epoch 739/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8650\n",
            "Epoch 740/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8640\n",
            "Epoch 741/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8629\n",
            "Epoch 742/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8619\n",
            "Epoch 743/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8609\n",
            "Epoch 744/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8598\n",
            "Epoch 745/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8588\n",
            "Epoch 746/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8578\n",
            "Epoch 747/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8567\n",
            "Epoch 748/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8557\n",
            "Epoch 749/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8547\n",
            "Epoch 750/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8537\n",
            "Epoch 751/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8526\n",
            "Epoch 752/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8516\n",
            "Epoch 753/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8506\n",
            "Epoch 754/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8496\n",
            "Epoch 755/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8486\n",
            "Epoch 756/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8476\n",
            "Epoch 757/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8466\n",
            "Epoch 758/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8456\n",
            "Epoch 759/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8446\n",
            "Epoch 760/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8436\n",
            "Epoch 761/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8426\n",
            "Epoch 762/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8416\n",
            "Epoch 763/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8406\n",
            "Epoch 764/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8396\n",
            "Epoch 765/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8386\n",
            "Epoch 766/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8376\n",
            "Epoch 767/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8366\n",
            "Epoch 768/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8356\n",
            "Epoch 769/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8346\n",
            "Epoch 770/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8336\n",
            "Epoch 771/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8327\n",
            "Epoch 772/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8317\n",
            "Epoch 773/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8307\n",
            "Epoch 774/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8297\n",
            "Epoch 775/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8288\n",
            "Epoch 776/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8278\n",
            "Epoch 777/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8268\n",
            "Epoch 778/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8259\n",
            "Epoch 779/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8249\n",
            "Epoch 780/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8239\n",
            "Epoch 781/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8230\n",
            "Epoch 782/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8220\n",
            "Epoch 783/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8211\n",
            "Epoch 784/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8201\n",
            "Epoch 785/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8191\n",
            "Epoch 786/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8182\n",
            "Epoch 787/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8172\n",
            "Epoch 788/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8163\n",
            "Epoch 789/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8153\n",
            "Epoch 790/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8144\n",
            "Epoch 791/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8135\n",
            "Epoch 792/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8125\n",
            "Epoch 793/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8116\n",
            "Epoch 794/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8106\n",
            "Epoch 795/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8097\n",
            "Epoch 796/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8088\n",
            "Epoch 797/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8078\n",
            "Epoch 798/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8069\n",
            "Epoch 799/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8060\n",
            "Epoch 800/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8050\n",
            "Epoch 801/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8041\n",
            "Epoch 802/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8032\n",
            "Epoch 803/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8023\n",
            "Epoch 804/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8013\n",
            "Epoch 805/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.8004\n",
            "Epoch 806/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7995\n",
            "Epoch 807/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7986\n",
            "Epoch 808/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7977\n",
            "Epoch 809/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7968\n",
            "Epoch 810/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7959\n",
            "Epoch 811/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7949\n",
            "Epoch 812/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7940\n",
            "Epoch 813/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7931\n",
            "Epoch 814/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7922\n",
            "Epoch 815/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7913\n",
            "Epoch 816/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7904\n",
            "Epoch 817/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7895\n",
            "Epoch 818/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7886\n",
            "Epoch 819/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7877\n",
            "Epoch 820/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7868\n",
            "Epoch 821/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7859\n",
            "Epoch 822/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7851\n",
            "Epoch 823/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7842\n",
            "Epoch 824/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7833\n",
            "Epoch 825/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7824\n",
            "Epoch 826/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7815\n",
            "Epoch 827/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7806\n",
            "Epoch 828/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7797\n",
            "Epoch 829/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7789\n",
            "Epoch 830/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7780\n",
            "Epoch 831/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7771\n",
            "Epoch 832/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7762\n",
            "Epoch 833/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7754\n",
            "Epoch 834/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7745\n",
            "Epoch 835/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7736\n",
            "Epoch 836/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7727\n",
            "Epoch 837/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7719\n",
            "Epoch 838/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7710\n",
            "Epoch 839/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7701\n",
            "Epoch 840/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7693\n",
            "Epoch 841/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7684\n",
            "Epoch 842/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7676\n",
            "Epoch 843/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7667\n",
            "Epoch 844/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7658\n",
            "Epoch 845/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7650\n",
            "Epoch 846/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7641\n",
            "Epoch 847/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7633\n",
            "Epoch 848/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7624\n",
            "Epoch 849/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7616\n",
            "Epoch 850/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7607\n",
            "Epoch 851/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7599\n",
            "Epoch 852/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7590\n",
            "Epoch 853/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7582\n",
            "Epoch 854/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7574\n",
            "Epoch 855/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7565\n",
            "Epoch 856/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7557\n",
            "Epoch 857/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7548\n",
            "Epoch 858/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7540\n",
            "Epoch 859/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7532\n",
            "Epoch 860/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7523\n",
            "Epoch 861/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7515\n",
            "Epoch 862/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7507\n",
            "Epoch 863/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7498\n",
            "Epoch 864/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7490\n",
            "Epoch 865/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7482\n",
            "Epoch 866/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7473\n",
            "Epoch 867/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7465\n",
            "Epoch 868/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7457\n",
            "Epoch 869/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7449\n",
            "Epoch 870/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7441\n",
            "Epoch 871/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7432\n",
            "Epoch 872/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7424\n",
            "Epoch 873/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7416\n",
            "Epoch 874/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7408\n",
            "Epoch 875/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7400\n",
            "Epoch 876/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7392\n",
            "Epoch 877/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7383\n",
            "Epoch 878/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7375\n",
            "Epoch 879/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7367\n",
            "Epoch 880/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7359\n",
            "Epoch 881/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7351\n",
            "Epoch 882/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7343\n",
            "Epoch 883/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7335\n",
            "Epoch 884/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7327\n",
            "Epoch 885/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7319\n",
            "Epoch 886/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7311\n",
            "Epoch 887/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7303\n",
            "Epoch 888/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7295\n",
            "Epoch 889/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7287\n",
            "Epoch 890/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7279\n",
            "Epoch 891/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7271\n",
            "Epoch 892/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7264\n",
            "Epoch 893/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7256\n",
            "Epoch 894/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7248\n",
            "Epoch 895/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7240\n",
            "Epoch 896/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7232\n",
            "Epoch 897/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7224\n",
            "Epoch 898/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7216\n",
            "Epoch 899/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7208\n",
            "Epoch 900/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7201\n",
            "Epoch 901/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7193\n",
            "Epoch 902/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7185\n",
            "Epoch 903/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7177\n",
            "Epoch 904/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7169\n",
            "Epoch 905/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7162\n",
            "Epoch 906/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7154\n",
            "Epoch 907/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7146\n",
            "Epoch 908/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7138\n",
            "Epoch 909/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7131\n",
            "Epoch 910/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7123\n",
            "Epoch 911/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7115\n",
            "Epoch 912/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7108\n",
            "Epoch 913/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7100\n",
            "Epoch 914/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7092\n",
            "Epoch 915/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7085\n",
            "Epoch 916/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7077\n",
            "Epoch 917/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7069\n",
            "Epoch 918/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7062\n",
            "Epoch 919/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7054\n",
            "Epoch 920/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7047\n",
            "Epoch 921/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7039\n",
            "Epoch 922/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7032\n",
            "Epoch 923/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7024\n",
            "Epoch 924/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7017\n",
            "Epoch 925/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7009\n",
            "Epoch 926/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.7001\n",
            "Epoch 927/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6994\n",
            "Epoch 928/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6986\n",
            "Epoch 929/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6979\n",
            "Epoch 930/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6971\n",
            "Epoch 931/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6964\n",
            "Epoch 932/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6956\n",
            "Epoch 933/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6949\n",
            "Epoch 934/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6941\n",
            "Epoch 935/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6934\n",
            "Epoch 936/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6926\n",
            "Epoch 937/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6919\n",
            "Epoch 938/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6911\n",
            "Epoch 939/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6904\n",
            "Epoch 940/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6896\n",
            "Epoch 941/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6889\n",
            "Epoch 942/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6881\n",
            "Epoch 943/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6874\n",
            "Epoch 944/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6867\n",
            "Epoch 945/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6859\n",
            "Epoch 946/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6852\n",
            "Epoch 947/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6844\n",
            "Epoch 948/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6837\n",
            "Epoch 949/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6829\n",
            "Epoch 950/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6822\n",
            "Epoch 951/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6815\n",
            "Epoch 952/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6807\n",
            "Epoch 953/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6800\n",
            "Epoch 954/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6793\n",
            "Epoch 955/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6785\n",
            "Epoch 956/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6778\n",
            "Epoch 957/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6771\n",
            "Epoch 958/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6764\n",
            "Epoch 959/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6756\n",
            "Epoch 960/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6749\n",
            "Epoch 961/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6742\n",
            "Epoch 962/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6735\n",
            "Epoch 963/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6728\n",
            "Epoch 964/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6720\n",
            "Epoch 965/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6713\n",
            "Epoch 966/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6706\n",
            "Epoch 967/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6699\n",
            "Epoch 968/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6692\n",
            "Epoch 969/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6684\n",
            "Epoch 970/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6677\n",
            "Epoch 971/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6670\n",
            "Epoch 972/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6663\n",
            "Epoch 973/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6656\n",
            "Epoch 974/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6649\n",
            "Epoch 975/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6642\n",
            "Epoch 976/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6635\n",
            "Epoch 977/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6628\n",
            "Epoch 978/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6621\n",
            "Epoch 979/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6614\n",
            "Epoch 980/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6607\n",
            "Epoch 981/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6600\n",
            "Epoch 982/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6593\n",
            "Epoch 983/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6586\n",
            "Epoch 984/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6579\n",
            "Epoch 985/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6572\n",
            "Epoch 986/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6565\n",
            "Epoch 987/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6558\n",
            "Epoch 988/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6551\n",
            "Epoch 989/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6544\n",
            "Epoch 990/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6537\n",
            "Epoch 991/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6530\n",
            "Epoch 992/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6523\n",
            "Epoch 993/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6516\n",
            "Epoch 994/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6509\n",
            "Epoch 995/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6503\n",
            "Epoch 996/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6496\n",
            "Epoch 997/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6489\n",
            "Epoch 998/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6482\n",
            "Epoch 999/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6475\n",
            "Epoch 1000/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6468\n",
            "Epoch 1001/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6462\n",
            "Epoch 1002/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6455\n",
            "Epoch 1003/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6448\n",
            "Epoch 1004/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6441\n",
            "Epoch 1005/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6435\n",
            "Epoch 1006/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6428\n",
            "Epoch 1007/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6421\n",
            "Epoch 1008/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6414\n",
            "Epoch 1009/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6408\n",
            "Epoch 1010/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6401\n",
            "Epoch 1011/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6394\n",
            "Epoch 1012/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6388\n",
            "Epoch 1013/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6381\n",
            "Epoch 1014/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6374\n",
            "Epoch 1015/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6368\n",
            "Epoch 1016/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6361\n",
            "Epoch 1017/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6355\n",
            "Epoch 1018/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6348\n",
            "Epoch 1019/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6341\n",
            "Epoch 1020/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6335\n",
            "Epoch 1021/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6328\n",
            "Epoch 1022/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6322\n",
            "Epoch 1023/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6315\n",
            "Epoch 1024/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6309\n",
            "Epoch 1025/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6302\n",
            "Epoch 1026/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6295\n",
            "Epoch 1027/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6289\n",
            "Epoch 1028/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6282\n",
            "Epoch 1029/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6276\n",
            "Epoch 1030/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6269\n",
            "Epoch 1031/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6263\n",
            "Epoch 1032/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6256\n",
            "Epoch 1033/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6250\n",
            "Epoch 1034/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6243\n",
            "Epoch 1035/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6237\n",
            "Epoch 1036/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6230\n",
            "Epoch 1037/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6224\n",
            "Epoch 1038/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6217\n",
            "Epoch 1039/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6211\n",
            "Epoch 1040/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6204\n",
            "Epoch 1041/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6198\n",
            "Epoch 1042/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6191\n",
            "Epoch 1043/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6185\n",
            "Epoch 1044/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6179\n",
            "Epoch 1045/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6172\n",
            "Epoch 1046/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6166\n",
            "Epoch 1047/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6159\n",
            "Epoch 1048/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6153\n",
            "Epoch 1049/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6147\n",
            "Epoch 1050/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6140\n",
            "Epoch 1051/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6134\n",
            "Epoch 1052/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6128\n",
            "Epoch 1053/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6122\n",
            "Epoch 1054/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6115\n",
            "Epoch 1055/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6109\n",
            "Epoch 1056/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6103\n",
            "Epoch 1057/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6097\n",
            "Epoch 1058/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6090\n",
            "Epoch 1059/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6084\n",
            "Epoch 1060/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6078\n",
            "Epoch 1061/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6072\n",
            "Epoch 1062/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6065\n",
            "Epoch 1063/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6059\n",
            "Epoch 1064/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6053\n",
            "Epoch 1065/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6047\n",
            "Epoch 1066/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6041\n",
            "Epoch 1067/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6035\n",
            "Epoch 1068/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6028\n",
            "Epoch 1069/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6022\n",
            "Epoch 1070/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6016\n",
            "Epoch 1071/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6010\n",
            "Epoch 1072/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.6004\n",
            "Epoch 1073/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5998\n",
            "Epoch 1074/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5992\n",
            "Epoch 1075/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5986\n",
            "Epoch 1076/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5980\n",
            "Epoch 1077/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5974\n",
            "Epoch 1078/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5968\n",
            "Epoch 1079/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5962\n",
            "Epoch 1080/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5955\n",
            "Epoch 1081/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5949\n",
            "Epoch 1082/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5943\n",
            "Epoch 1083/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5937\n",
            "Epoch 1084/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5931\n",
            "Epoch 1085/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5925\n",
            "Epoch 1086/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5919\n",
            "Epoch 1087/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5913\n",
            "Epoch 1088/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5907\n",
            "Epoch 1089/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5902\n",
            "Epoch 1090/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5896\n",
            "Epoch 1091/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5890\n",
            "Epoch 1092/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5884\n",
            "Epoch 1093/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5878\n",
            "Epoch 1094/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5872\n",
            "Epoch 1095/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5866\n",
            "Epoch 1096/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5860\n",
            "Epoch 1097/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5854\n",
            "Epoch 1098/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5848\n",
            "Epoch 1099/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5842\n",
            "Epoch 1100/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5836\n",
            "Epoch 1101/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5830\n",
            "Epoch 1102/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5824\n",
            "Epoch 1103/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5819\n",
            "Epoch 1104/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5813\n",
            "Epoch 1105/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5807\n",
            "Epoch 1106/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5801\n",
            "Epoch 1107/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5795\n",
            "Epoch 1108/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5790\n",
            "Epoch 1109/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5784\n",
            "Epoch 1110/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5778\n",
            "Epoch 1111/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5772\n",
            "Epoch 1112/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5766\n",
            "Epoch 1113/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5761\n",
            "Epoch 1114/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5755\n",
            "Epoch 1115/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5749\n",
            "Epoch 1116/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5743\n",
            "Epoch 1117/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5738\n",
            "Epoch 1118/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5732\n",
            "Epoch 1119/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5726\n",
            "Epoch 1120/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5721\n",
            "Epoch 1121/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5715\n",
            "Epoch 1122/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5709\n",
            "Epoch 1123/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5704\n",
            "Epoch 1124/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5698\n",
            "Epoch 1125/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5692\n",
            "Epoch 1126/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5687\n",
            "Epoch 1127/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5681\n",
            "Epoch 1128/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5675\n",
            "Epoch 1129/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5670\n",
            "Epoch 1130/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5664\n",
            "Epoch 1131/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5658\n",
            "Epoch 1132/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5653\n",
            "Epoch 1133/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5647\n",
            "Epoch 1134/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5642\n",
            "Epoch 1135/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5636\n",
            "Epoch 1136/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5631\n",
            "Epoch 1137/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5625\n",
            "Epoch 1138/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5619\n",
            "Epoch 1139/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5614\n",
            "Epoch 1140/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5608\n",
            "Epoch 1141/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5603\n",
            "Epoch 1142/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5597\n",
            "Epoch 1143/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5592\n",
            "Epoch 1144/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5586\n",
            "Epoch 1145/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5581\n",
            "Epoch 1146/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5575\n",
            "Epoch 1147/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5570\n",
            "Epoch 1148/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5564\n",
            "Epoch 1149/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5559\n",
            "Epoch 1150/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5554\n",
            "Epoch 1151/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5548\n",
            "Epoch 1152/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5543\n",
            "Epoch 1153/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5537\n",
            "Epoch 1154/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5532\n",
            "Epoch 1155/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5526\n",
            "Epoch 1156/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5521\n",
            "Epoch 1157/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5516\n",
            "Epoch 1158/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5510\n",
            "Epoch 1159/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5505\n",
            "Epoch 1160/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5499\n",
            "Epoch 1161/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5494\n",
            "Epoch 1162/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5489\n",
            "Epoch 1163/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5483\n",
            "Epoch 1164/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5478\n",
            "Epoch 1165/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5473\n",
            "Epoch 1166/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5467\n",
            "Epoch 1167/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5462\n",
            "Epoch 1168/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5457\n",
            "Epoch 1169/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5451\n",
            "Epoch 1170/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5446\n",
            "Epoch 1171/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5441\n",
            "Epoch 1172/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5436\n",
            "Epoch 1173/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5430\n",
            "Epoch 1174/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5425\n",
            "Epoch 1175/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5420\n",
            "Epoch 1176/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5415\n",
            "Epoch 1177/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5409\n",
            "Epoch 1178/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5404\n",
            "Epoch 1179/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5399\n",
            "Epoch 1180/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5394\n",
            "Epoch 1181/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5388\n",
            "Epoch 1182/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5383\n",
            "Epoch 1183/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5378\n",
            "Epoch 1184/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5373\n",
            "Epoch 1185/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5368\n",
            "Epoch 1186/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5362\n",
            "Epoch 1187/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5357\n",
            "Epoch 1188/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5352\n",
            "Epoch 1189/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5347\n",
            "Epoch 1190/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5342\n",
            "Epoch 1191/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5337\n",
            "Epoch 1192/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5332\n",
            "Epoch 1193/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5327\n",
            "Epoch 1194/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5321\n",
            "Epoch 1195/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5316\n",
            "Epoch 1196/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5311\n",
            "Epoch 1197/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5306\n",
            "Epoch 1198/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5301\n",
            "Epoch 1199/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5296\n",
            "Epoch 1200/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5291\n",
            "Epoch 1201/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5286\n",
            "Epoch 1202/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5281\n",
            "Epoch 1203/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5276\n",
            "Epoch 1204/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5271\n",
            "Epoch 1205/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5266\n",
            "Epoch 1206/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5261\n",
            "Epoch 1207/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5256\n",
            "Epoch 1208/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5251\n",
            "Epoch 1209/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5246\n",
            "Epoch 1210/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5241\n",
            "Epoch 1211/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5236\n",
            "Epoch 1212/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5231\n",
            "Epoch 1213/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5226\n",
            "Epoch 1214/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5221\n",
            "Epoch 1215/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5216\n",
            "Epoch 1216/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5211\n",
            "Epoch 1217/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5206\n",
            "Epoch 1218/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5201\n",
            "Epoch 1219/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5196\n",
            "Epoch 1220/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5191\n",
            "Epoch 1221/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5186\n",
            "Epoch 1222/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5181\n",
            "Epoch 1223/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5176\n",
            "Epoch 1224/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5172\n",
            "Epoch 1225/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5167\n",
            "Epoch 1226/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5162\n",
            "Epoch 1227/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5157\n",
            "Epoch 1228/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5152\n",
            "Epoch 1229/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5147\n",
            "Epoch 1230/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5142\n",
            "Epoch 1231/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5137\n",
            "Epoch 1232/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5133\n",
            "Epoch 1233/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5128\n",
            "Epoch 1234/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5123\n",
            "Epoch 1235/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5118\n",
            "Epoch 1236/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5113\n",
            "Epoch 1237/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5109\n",
            "Epoch 1238/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5104\n",
            "Epoch 1239/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5099\n",
            "Epoch 1240/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5094\n",
            "Epoch 1241/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5089\n",
            "Epoch 1242/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5085\n",
            "Epoch 1243/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5080\n",
            "Epoch 1244/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5075\n",
            "Epoch 1245/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5070\n",
            "Epoch 1246/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5066\n",
            "Epoch 1247/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5061\n",
            "Epoch 1248/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5056\n",
            "Epoch 1249/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5051\n",
            "Epoch 1250/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5047\n",
            "Epoch 1251/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5042\n",
            "Epoch 1252/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5037\n",
            "Epoch 1253/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5033\n",
            "Epoch 1254/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5028\n",
            "Epoch 1255/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5023\n",
            "Epoch 1256/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5019\n",
            "Epoch 1257/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5014\n",
            "Epoch 1258/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5009\n",
            "Epoch 1259/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5005\n",
            "Epoch 1260/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.5000\n",
            "Epoch 1261/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4995\n",
            "Epoch 1262/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4991\n",
            "Epoch 1263/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4986\n",
            "Epoch 1264/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4981\n",
            "Epoch 1265/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4977\n",
            "Epoch 1266/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4972\n",
            "Epoch 1267/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4967\n",
            "Epoch 1268/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4963\n",
            "Epoch 1269/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4958\n",
            "Epoch 1270/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4954\n",
            "Epoch 1271/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4949\n",
            "Epoch 1272/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4945\n",
            "Epoch 1273/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4940\n",
            "Epoch 1274/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4935\n",
            "Epoch 1275/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4931\n",
            "Epoch 1276/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4926\n",
            "Epoch 1277/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4922\n",
            "Epoch 1278/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4917\n",
            "Epoch 1279/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4913\n",
            "Epoch 1280/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4908\n",
            "Epoch 1281/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4904\n",
            "Epoch 1282/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4899\n",
            "Epoch 1283/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4895\n",
            "Epoch 1284/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4890\n",
            "Epoch 1285/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4886\n",
            "Epoch 1286/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4881\n",
            "Epoch 1287/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4877\n",
            "Epoch 1288/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4872\n",
            "Epoch 1289/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4868\n",
            "Epoch 1290/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4863\n",
            "Epoch 1291/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4859\n",
            "Epoch 1292/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4854\n",
            "Epoch 1293/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4850\n",
            "Epoch 1294/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4845\n",
            "Epoch 1295/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4841\n",
            "Epoch 1296/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4836\n",
            "Epoch 1297/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4832\n",
            "Epoch 1298/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4827\n",
            "Epoch 1299/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4823\n",
            "Epoch 1300/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4819\n",
            "Epoch 1301/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4814\n",
            "Epoch 1302/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4810\n",
            "Epoch 1303/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4805\n",
            "Epoch 1304/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4801\n",
            "Epoch 1305/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4796\n",
            "Epoch 1306/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4792\n",
            "Epoch 1307/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4788\n",
            "Epoch 1308/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4783\n",
            "Epoch 1309/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4779\n",
            "Epoch 1310/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4774\n",
            "Epoch 1311/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4770\n",
            "Epoch 1312/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4766\n",
            "Epoch 1313/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4761\n",
            "Epoch 1314/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4757\n",
            "Epoch 1315/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4753\n",
            "Epoch 1316/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4748\n",
            "Epoch 1317/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4744\n",
            "Epoch 1318/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4740\n",
            "Epoch 1319/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4735\n",
            "Epoch 1320/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4731\n",
            "Epoch 1321/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4727\n",
            "Epoch 1322/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4722\n",
            "Epoch 1323/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4718\n",
            "Epoch 1324/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4714\n",
            "Epoch 1325/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4709\n",
            "Epoch 1326/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4705\n",
            "Epoch 1327/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4701\n",
            "Epoch 1328/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4697\n",
            "Epoch 1329/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4692\n",
            "Epoch 1330/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4688\n",
            "Epoch 1331/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4684\n",
            "Epoch 1332/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4680\n",
            "Epoch 1333/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4675\n",
            "Epoch 1334/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4671\n",
            "Epoch 1335/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4667\n",
            "Epoch 1336/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4663\n",
            "Epoch 1337/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4658\n",
            "Epoch 1338/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4654\n",
            "Epoch 1339/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4650\n",
            "Epoch 1340/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4646\n",
            "Epoch 1341/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4642\n",
            "Epoch 1342/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4637\n",
            "Epoch 1343/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4633\n",
            "Epoch 1344/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4629\n",
            "Epoch 1345/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4625\n",
            "Epoch 1346/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4621\n",
            "Epoch 1347/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4616\n",
            "Epoch 1348/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4612\n",
            "Epoch 1349/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4608\n",
            "Epoch 1350/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4604\n",
            "Epoch 1351/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4600\n",
            "Epoch 1352/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4596\n",
            "Epoch 1353/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4592\n",
            "Epoch 1354/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4587\n",
            "Epoch 1355/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4583\n",
            "Epoch 1356/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4579\n",
            "Epoch 1357/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4575\n",
            "Epoch 1358/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4571\n",
            "Epoch 1359/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4567\n",
            "Epoch 1360/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4563\n",
            "Epoch 1361/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4559\n",
            "Epoch 1362/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4555\n",
            "Epoch 1363/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4550\n",
            "Epoch 1364/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4546\n",
            "Epoch 1365/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4542\n",
            "Epoch 1366/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4538\n",
            "Epoch 1367/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4534\n",
            "Epoch 1368/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4530\n",
            "Epoch 1369/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4526\n",
            "Epoch 1370/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4522\n",
            "Epoch 1371/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4518\n",
            "Epoch 1372/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4514\n",
            "Epoch 1373/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4510\n",
            "Epoch 1374/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4506\n",
            "Epoch 1375/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4502\n",
            "Epoch 1376/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4498\n",
            "Epoch 1377/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4494\n",
            "Epoch 1378/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4490\n",
            "Epoch 1379/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4486\n",
            "Epoch 1380/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4482\n",
            "Epoch 1381/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4478\n",
            "Epoch 1382/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4474\n",
            "Epoch 1383/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4470\n",
            "Epoch 1384/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4466\n",
            "Epoch 1385/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4462\n",
            "Epoch 1386/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4458\n",
            "Epoch 1387/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4454\n",
            "Epoch 1388/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4450\n",
            "Epoch 1389/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4446\n",
            "Epoch 1390/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4442\n",
            "Epoch 1391/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4438\n",
            "Epoch 1392/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4434\n",
            "Epoch 1393/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4430\n",
            "Epoch 1394/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4426\n",
            "Epoch 1395/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4422\n",
            "Epoch 1396/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4418\n",
            "Epoch 1397/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4415\n",
            "Epoch 1398/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4411\n",
            "Epoch 1399/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4407\n",
            "Epoch 1400/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4403\n",
            "Epoch 1401/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4399\n",
            "Epoch 1402/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4395\n",
            "Epoch 1403/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4391\n",
            "Epoch 1404/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4387\n",
            "Epoch 1405/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4383\n",
            "Epoch 1406/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4379\n",
            "Epoch 1407/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4376\n",
            "Epoch 1408/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4372\n",
            "Epoch 1409/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4368\n",
            "Epoch 1410/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4364\n",
            "Epoch 1411/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4360\n",
            "Epoch 1412/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4356\n",
            "Epoch 1413/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4352\n",
            "Epoch 1414/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4349\n",
            "Epoch 1415/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4345\n",
            "Epoch 1416/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4341\n",
            "Epoch 1417/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4337\n",
            "Epoch 1418/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4333\n",
            "Epoch 1419/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4330\n",
            "Epoch 1420/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4326\n",
            "Epoch 1421/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4322\n",
            "Epoch 1422/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4318\n",
            "Epoch 1423/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4314\n",
            "Epoch 1424/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4311\n",
            "Epoch 1425/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4307\n",
            "Epoch 1426/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4303\n",
            "Epoch 1427/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4299\n",
            "Epoch 1428/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4295\n",
            "Epoch 1429/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4292\n",
            "Epoch 1430/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4288\n",
            "Epoch 1431/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4284\n",
            "Epoch 1432/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4280\n",
            "Epoch 1433/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4277\n",
            "Epoch 1434/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4273\n",
            "Epoch 1435/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4269\n",
            "Epoch 1436/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4266\n",
            "Epoch 1437/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4262\n",
            "Epoch 1438/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4258\n",
            "Epoch 1439/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4254\n",
            "Epoch 1440/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4251\n",
            "Epoch 1441/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4247\n",
            "Epoch 1442/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4243\n",
            "Epoch 1443/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4240\n",
            "Epoch 1444/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4236\n",
            "Epoch 1445/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4232\n",
            "Epoch 1446/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4228\n",
            "Epoch 1447/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4225\n",
            "Epoch 1448/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4221\n",
            "Epoch 1449/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4217\n",
            "Epoch 1450/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4214\n",
            "Epoch 1451/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4210\n",
            "Epoch 1452/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4206\n",
            "Epoch 1453/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4203\n",
            "Epoch 1454/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4199\n",
            "Epoch 1455/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4195\n",
            "Epoch 1456/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4192\n",
            "Epoch 1457/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4188\n",
            "Epoch 1458/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4185\n",
            "Epoch 1459/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4181\n",
            "Epoch 1460/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4177\n",
            "Epoch 1461/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4174\n",
            "Epoch 1462/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4170\n",
            "Epoch 1463/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4167\n",
            "Epoch 1464/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4163\n",
            "Epoch 1465/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4159\n",
            "Epoch 1466/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4156\n",
            "Epoch 1467/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4152\n",
            "Epoch 1468/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4149\n",
            "Epoch 1469/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4145\n",
            "Epoch 1470/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4141\n",
            "Epoch 1471/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4138\n",
            "Epoch 1472/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4134\n",
            "Epoch 1473/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4131\n",
            "Epoch 1474/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4127\n",
            "Epoch 1475/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4124\n",
            "Epoch 1476/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4120\n",
            "Epoch 1477/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4116\n",
            "Epoch 1478/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4113\n",
            "Epoch 1479/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4109\n",
            "Epoch 1480/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4106\n",
            "Epoch 1481/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4102\n",
            "Epoch 1482/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4099\n",
            "Epoch 1483/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4095\n",
            "Epoch 1484/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4092\n",
            "Epoch 1485/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4088\n",
            "Epoch 1486/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4085\n",
            "Epoch 1487/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4081\n",
            "Epoch 1488/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4078\n",
            "Epoch 1489/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4074\n",
            "Epoch 1490/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4071\n",
            "Epoch 1491/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4067\n",
            "Epoch 1492/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4064\n",
            "Epoch 1493/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4060\n",
            "Epoch 1494/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4057\n",
            "Epoch 1495/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4053\n",
            "Epoch 1496/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4050\n",
            "Epoch 1497/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4046\n",
            "Epoch 1498/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4043\n",
            "Epoch 1499/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4040\n",
            "Epoch 1500/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4036\n",
            "Epoch 1501/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4033\n",
            "Epoch 1502/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4029\n",
            "Epoch 1503/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4026\n",
            "Epoch 1504/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4022\n",
            "Epoch 1505/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4019\n",
            "Epoch 1506/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4015\n",
            "Epoch 1507/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4012\n",
            "Epoch 1508/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4009\n",
            "Epoch 1509/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4005\n",
            "Epoch 1510/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.4002\n",
            "Epoch 1511/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3998\n",
            "Epoch 1512/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3995\n",
            "Epoch 1513/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3992\n",
            "Epoch 1514/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3988\n",
            "Epoch 1515/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3985\n",
            "Epoch 1516/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3981\n",
            "Epoch 1517/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3978\n",
            "Epoch 1518/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3975\n",
            "Epoch 1519/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3971\n",
            "Epoch 1520/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3968\n",
            "Epoch 1521/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3965\n",
            "Epoch 1522/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3961\n",
            "Epoch 1523/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3958\n",
            "Epoch 1524/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3955\n",
            "Epoch 1525/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3951\n",
            "Epoch 1526/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3948\n",
            "Epoch 1527/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3945\n",
            "Epoch 1528/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3941\n",
            "Epoch 1529/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3938\n",
            "Epoch 1530/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3935\n",
            "Epoch 1531/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3931\n",
            "Epoch 1532/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3928\n",
            "Epoch 1533/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3925\n",
            "Epoch 1534/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3921\n",
            "Epoch 1535/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3918\n",
            "Epoch 1536/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3915\n",
            "Epoch 1537/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3911\n",
            "Epoch 1538/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3908\n",
            "Epoch 1539/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3905\n",
            "Epoch 1540/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3901\n",
            "Epoch 1541/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3898\n",
            "Epoch 1542/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3895\n",
            "Epoch 1543/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3892\n",
            "Epoch 1544/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3888\n",
            "Epoch 1545/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3885\n",
            "Epoch 1546/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3882\n",
            "Epoch 1547/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3879\n",
            "Epoch 1548/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3875\n",
            "Epoch 1549/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3872\n",
            "Epoch 1550/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3869\n",
            "Epoch 1551/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3866\n",
            "Epoch 1552/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3862\n",
            "Epoch 1553/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3859\n",
            "Epoch 1554/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3856\n",
            "Epoch 1555/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3853\n",
            "Epoch 1556/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3849\n",
            "Epoch 1557/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3846\n",
            "Epoch 1558/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3843\n",
            "Epoch 1559/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3840\n",
            "Epoch 1560/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3837\n",
            "Epoch 1561/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3833\n",
            "Epoch 1562/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3830\n",
            "Epoch 1563/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3827\n",
            "Epoch 1564/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3824\n",
            "Epoch 1565/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3820\n",
            "Epoch 1566/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3817\n",
            "Epoch 1567/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3814\n",
            "Epoch 1568/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3811\n",
            "Epoch 1569/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3808\n",
            "Epoch 1570/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3805\n",
            "Epoch 1571/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3801\n",
            "Epoch 1572/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3798\n",
            "Epoch 1573/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3795\n",
            "Epoch 1574/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3792\n",
            "Epoch 1575/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3789\n",
            "Epoch 1576/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3785\n",
            "Epoch 1577/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3782\n",
            "Epoch 1578/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3779\n",
            "Epoch 1579/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3776\n",
            "Epoch 1580/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3773\n",
            "Epoch 1581/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3769\n",
            "Epoch 1582/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3766\n",
            "Epoch 1583/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3763\n",
            "Epoch 1584/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3760\n",
            "Epoch 1585/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3757\n",
            "Epoch 1586/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3754\n",
            "Epoch 1587/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3750\n",
            "Epoch 1588/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3747\n",
            "Epoch 1589/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3744\n",
            "Epoch 1590/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3741\n",
            "Epoch 1591/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3738\n",
            "Epoch 1592/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3735\n",
            "Epoch 1593/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3732\n",
            "Epoch 1594/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3729\n",
            "Epoch 1595/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3725\n",
            "Epoch 1596/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3722\n",
            "Epoch 1597/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3719\n",
            "Epoch 1598/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3716\n",
            "Epoch 1599/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3713\n",
            "Epoch 1600/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3710\n",
            "Epoch 1601/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3707\n",
            "Epoch 1602/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3704\n",
            "Epoch 1603/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3701\n",
            "Epoch 1604/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3698\n",
            "Epoch 1605/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3695\n",
            "Epoch 1606/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3692\n",
            "Epoch 1607/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3689\n",
            "Epoch 1608/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3685\n",
            "Epoch 1609/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3682\n",
            "Epoch 1610/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3679\n",
            "Epoch 1611/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3676\n",
            "Epoch 1612/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3673\n",
            "Epoch 1613/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3670\n",
            "Epoch 1614/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3667\n",
            "Epoch 1615/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3664\n",
            "Epoch 1616/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3661\n",
            "Epoch 1617/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3658\n",
            "Epoch 1618/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3655\n",
            "Epoch 1619/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3652\n",
            "Epoch 1620/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3649\n",
            "Epoch 1621/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3646\n",
            "Epoch 1622/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3643\n",
            "Epoch 1623/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3640\n",
            "Epoch 1624/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3637\n",
            "Epoch 1625/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3634\n",
            "Epoch 1626/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3631\n",
            "Epoch 1627/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3628\n",
            "Epoch 1628/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3625\n",
            "Epoch 1629/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3622\n",
            "Epoch 1630/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3619\n",
            "Epoch 1631/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3616\n",
            "Epoch 1632/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3613\n",
            "Epoch 1633/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3610\n",
            "Epoch 1634/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3607\n",
            "Epoch 1635/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3604\n",
            "Epoch 1636/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3601\n",
            "Epoch 1637/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3598\n",
            "Epoch 1638/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3595\n",
            "Epoch 1639/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3592\n",
            "Epoch 1640/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3589\n",
            "Epoch 1641/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3587\n",
            "Epoch 1642/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3584\n",
            "Epoch 1643/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3581\n",
            "Epoch 1644/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3578\n",
            "Epoch 1645/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3575\n",
            "Epoch 1646/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3572\n",
            "Epoch 1647/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3569\n",
            "Epoch 1648/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3566\n",
            "Epoch 1649/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3563\n",
            "Epoch 1650/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3560\n",
            "Epoch 1651/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3557\n",
            "Epoch 1652/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3554\n",
            "Epoch 1653/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3551\n",
            "Epoch 1654/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3549\n",
            "Epoch 1655/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3546\n",
            "Epoch 1656/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3543\n",
            "Epoch 1657/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3540\n",
            "Epoch 1658/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3537\n",
            "Epoch 1659/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3534\n",
            "Epoch 1660/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3531\n",
            "Epoch 1661/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3528\n",
            "Epoch 1662/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3525\n",
            "Epoch 1663/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3523\n",
            "Epoch 1664/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3520\n",
            "Epoch 1665/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3517\n",
            "Epoch 1666/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3514\n",
            "Epoch 1667/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3511\n",
            "Epoch 1668/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3508\n",
            "Epoch 1669/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3505\n",
            "Epoch 1670/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3503\n",
            "Epoch 1671/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3500\n",
            "Epoch 1672/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3497\n",
            "Epoch 1673/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3494\n",
            "Epoch 1674/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3491\n",
            "Epoch 1675/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3488\n",
            "Epoch 1676/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3486\n",
            "Epoch 1677/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3483\n",
            "Epoch 1678/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3480\n",
            "Epoch 1679/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3477\n",
            "Epoch 1680/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3474\n",
            "Epoch 1681/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3471\n",
            "Epoch 1682/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3469\n",
            "Epoch 1683/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3466\n",
            "Epoch 1684/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3463\n",
            "Epoch 1685/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3460\n",
            "Epoch 1686/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3457\n",
            "Epoch 1687/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3455\n",
            "Epoch 1688/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3452\n",
            "Epoch 1689/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3449\n",
            "Epoch 1690/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3446\n",
            "Epoch 1691/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3443\n",
            "Epoch 1692/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3441\n",
            "Epoch 1693/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3438\n",
            "Epoch 1694/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3435\n",
            "Epoch 1695/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3432\n",
            "Epoch 1696/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3430\n",
            "Epoch 1697/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3427\n",
            "Epoch 1698/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3424\n",
            "Epoch 1699/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3421\n",
            "Epoch 1700/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3419\n",
            "Epoch 1701/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3416\n",
            "Epoch 1702/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3413\n",
            "Epoch 1703/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3410\n",
            "Epoch 1704/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3408\n",
            "Epoch 1705/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3405\n",
            "Epoch 1706/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3402\n",
            "Epoch 1707/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3399\n",
            "Epoch 1708/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3397\n",
            "Epoch 1709/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3394\n",
            "Epoch 1710/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3391\n",
            "Epoch 1711/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3388\n",
            "Epoch 1712/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3386\n",
            "Epoch 1713/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3383\n",
            "Epoch 1714/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3380\n",
            "Epoch 1715/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3378\n",
            "Epoch 1716/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3375\n",
            "Epoch 1717/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3372\n",
            "Epoch 1718/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3369\n",
            "Epoch 1719/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3367\n",
            "Epoch 1720/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3364\n",
            "Epoch 1721/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3361\n",
            "Epoch 1722/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3359\n",
            "Epoch 1723/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3356\n",
            "Epoch 1724/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3353\n",
            "Epoch 1725/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3351\n",
            "Epoch 1726/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3348\n",
            "Epoch 1727/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3345\n",
            "Epoch 1728/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3343\n",
            "Epoch 1729/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3340\n",
            "Epoch 1730/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3337\n",
            "Epoch 1731/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3335\n",
            "Epoch 1732/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3332\n",
            "Epoch 1733/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3329\n",
            "Epoch 1734/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3327\n",
            "Epoch 1735/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3324\n",
            "Epoch 1736/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3321\n",
            "Epoch 1737/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3319\n",
            "Epoch 1738/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3316\n",
            "Epoch 1739/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3313\n",
            "Epoch 1740/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3311\n",
            "Epoch 1741/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3308\n",
            "Epoch 1742/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3306\n",
            "Epoch 1743/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3303\n",
            "Epoch 1744/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3300\n",
            "Epoch 1745/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3298\n",
            "Epoch 1746/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3295\n",
            "Epoch 1747/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3292\n",
            "Epoch 1748/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3290\n",
            "Epoch 1749/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3287\n",
            "Epoch 1750/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3285\n",
            "Epoch 1751/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3282\n",
            "Epoch 1752/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3279\n",
            "Epoch 1753/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3277\n",
            "Epoch 1754/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3274\n",
            "Epoch 1755/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3272\n",
            "Epoch 1756/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3269\n",
            "Epoch 1757/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3267\n",
            "Epoch 1758/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3264\n",
            "Epoch 1759/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3261\n",
            "Epoch 1760/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3259\n",
            "Epoch 1761/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3256\n",
            "Epoch 1762/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3254\n",
            "Epoch 1763/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3251\n",
            "Epoch 1764/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3249\n",
            "Epoch 1765/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3246\n",
            "Epoch 1766/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3243\n",
            "Epoch 1767/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3241\n",
            "Epoch 1768/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3238\n",
            "Epoch 1769/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3236\n",
            "Epoch 1770/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3233\n",
            "Epoch 1771/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3231\n",
            "Epoch 1772/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3228\n",
            "Epoch 1773/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3226\n",
            "Epoch 1774/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3223\n",
            "Epoch 1775/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3221\n",
            "Epoch 1776/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3218\n",
            "Epoch 1777/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3215\n",
            "Epoch 1778/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3213\n",
            "Epoch 1779/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3210\n",
            "Epoch 1780/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3208\n",
            "Epoch 1781/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3205\n",
            "Epoch 1782/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3203\n",
            "Epoch 1783/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3200\n",
            "Epoch 1784/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3198\n",
            "Epoch 1785/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3195\n",
            "Epoch 1786/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3193\n",
            "Epoch 1787/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3190\n",
            "Epoch 1788/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3188\n",
            "Epoch 1789/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3185\n",
            "Epoch 1790/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3183\n",
            "Epoch 1791/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3180\n",
            "Epoch 1792/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3178\n",
            "Epoch 1793/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3176\n",
            "Epoch 1794/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3173\n",
            "Epoch 1795/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3171\n",
            "Epoch 1796/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3168\n",
            "Epoch 1797/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3166\n",
            "Epoch 1798/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3163\n",
            "Epoch 1799/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3161\n",
            "Epoch 1800/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3158\n",
            "Epoch 1801/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3156\n",
            "Epoch 1802/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3153\n",
            "Epoch 1803/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3151\n",
            "Epoch 1804/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3148\n",
            "Epoch 1805/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3146\n",
            "Epoch 1806/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3144\n",
            "Epoch 1807/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3141\n",
            "Epoch 1808/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3139\n",
            "Epoch 1809/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3136\n",
            "Epoch 1810/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3134\n",
            "Epoch 1811/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3131\n",
            "Epoch 1812/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3129\n",
            "Epoch 1813/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3127\n",
            "Epoch 1814/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3124\n",
            "Epoch 1815/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3122\n",
            "Epoch 1816/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3119\n",
            "Epoch 1817/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3117\n",
            "Epoch 1818/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3114\n",
            "Epoch 1819/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3112\n",
            "Epoch 1820/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3110\n",
            "Epoch 1821/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3107\n",
            "Epoch 1822/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3105\n",
            "Epoch 1823/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3102\n",
            "Epoch 1824/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3100\n",
            "Epoch 1825/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3098\n",
            "Epoch 1826/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3095\n",
            "Epoch 1827/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3093\n",
            "Epoch 1828/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3091\n",
            "Epoch 1829/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3088\n",
            "Epoch 1830/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3086\n",
            "Epoch 1831/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3083\n",
            "Epoch 1832/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3081\n",
            "Epoch 1833/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3079\n",
            "Epoch 1834/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3076\n",
            "Epoch 1835/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3074\n",
            "Epoch 1836/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3072\n",
            "Epoch 1837/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3069\n",
            "Epoch 1838/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3067\n",
            "Epoch 1839/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3064\n",
            "Epoch 1840/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3062\n",
            "Epoch 1841/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3060\n",
            "Epoch 1842/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3057\n",
            "Epoch 1843/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3055\n",
            "Epoch 1844/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3053\n",
            "Epoch 1845/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3050\n",
            "Epoch 1846/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3048\n",
            "Epoch 1847/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3046\n",
            "Epoch 1848/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3043\n",
            "Epoch 1849/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3041\n",
            "Epoch 1850/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3039\n",
            "Epoch 1851/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3036\n",
            "Epoch 1852/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3034\n",
            "Epoch 1853/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3032\n",
            "Epoch 1854/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3029\n",
            "Epoch 1855/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3027\n",
            "Epoch 1856/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3025\n",
            "Epoch 1857/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3022\n",
            "Epoch 1858/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3020\n",
            "Epoch 1859/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3018\n",
            "Epoch 1860/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3015\n",
            "Epoch 1861/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3013\n",
            "Epoch 1862/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3011\n",
            "Epoch 1863/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3008\n",
            "Epoch 1864/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3006\n",
            "Epoch 1865/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3004\n",
            "Epoch 1866/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.3001\n",
            "Epoch 1867/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2999\n",
            "Epoch 1868/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2997\n",
            "Epoch 1869/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2995\n",
            "Epoch 1870/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2992\n",
            "Epoch 1871/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2990\n",
            "Epoch 1872/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2988\n",
            "Epoch 1873/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2985\n",
            "Epoch 1874/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2983\n",
            "Epoch 1875/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2981\n",
            "Epoch 1876/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2979\n",
            "Epoch 1877/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2976\n",
            "Epoch 1878/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2974\n",
            "Epoch 1879/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2972\n",
            "Epoch 1880/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2970\n",
            "Epoch 1881/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2967\n",
            "Epoch 1882/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2965\n",
            "Epoch 1883/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2963\n",
            "Epoch 1884/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2961\n",
            "Epoch 1885/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2958\n",
            "Epoch 1886/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2956\n",
            "Epoch 1887/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2954\n",
            "Epoch 1888/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2952\n",
            "Epoch 1889/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2949\n",
            "Epoch 1890/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2947\n",
            "Epoch 1891/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2945\n",
            "Epoch 1892/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2943\n",
            "Epoch 1893/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2940\n",
            "Epoch 1894/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2938\n",
            "Epoch 1895/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2936\n",
            "Epoch 1896/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2934\n",
            "Epoch 1897/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2932\n",
            "Epoch 1898/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2929\n",
            "Epoch 1899/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2927\n",
            "Epoch 1900/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2925\n",
            "Epoch 1901/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2923\n",
            "Epoch 1902/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2921\n",
            "Epoch 1903/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2918\n",
            "Epoch 1904/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2916\n",
            "Epoch 1905/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2914\n",
            "Epoch 1906/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2912\n",
            "Epoch 1907/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2910\n",
            "Epoch 1908/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2907\n",
            "Epoch 1909/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2905\n",
            "Epoch 1910/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2903\n",
            "Epoch 1911/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2901\n",
            "Epoch 1912/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2899\n",
            "Epoch 1913/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2897\n",
            "Epoch 1914/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2894\n",
            "Epoch 1915/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2892\n",
            "Epoch 1916/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2890\n",
            "Epoch 1917/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2888\n",
            "Epoch 1918/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2886\n",
            "Epoch 1919/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2884\n",
            "Epoch 1920/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2881\n",
            "Epoch 1921/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2879\n",
            "Epoch 1922/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2877\n",
            "Epoch 1923/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2875\n",
            "Epoch 1924/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2873\n",
            "Epoch 1925/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2871\n",
            "Epoch 1926/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2868\n",
            "Epoch 1927/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2866\n",
            "Epoch 1928/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2864\n",
            "Epoch 1929/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2862\n",
            "Epoch 1930/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2860\n",
            "Epoch 1931/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2858\n",
            "Epoch 1932/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2855\n",
            "Epoch 1933/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2853\n",
            "Epoch 1934/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2851\n",
            "Epoch 1935/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2849\n",
            "Epoch 1936/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2847\n",
            "Epoch 1937/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2845\n",
            "Epoch 1938/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2843\n",
            "Epoch 1939/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2841\n",
            "Epoch 1940/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2838\n",
            "Epoch 1941/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2836\n",
            "Epoch 1942/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2834\n",
            "Epoch 1943/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2832\n",
            "Epoch 1944/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2830\n",
            "Epoch 1945/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2828\n",
            "Epoch 1946/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2826\n",
            "Epoch 1947/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2824\n",
            "Epoch 1948/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2822\n",
            "Epoch 1949/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2819\n",
            "Epoch 1950/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2817\n",
            "Epoch 1951/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2815\n",
            "Epoch 1952/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2813\n",
            "Epoch 1953/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2811\n",
            "Epoch 1954/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2809\n",
            "Epoch 1955/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2807\n",
            "Epoch 1956/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2805\n",
            "Epoch 1957/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2803\n",
            "Epoch 1958/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2801\n",
            "Epoch 1959/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2799\n",
            "Epoch 1960/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2797\n",
            "Epoch 1961/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2794\n",
            "Epoch 1962/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2792\n",
            "Epoch 1963/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2790\n",
            "Epoch 1964/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2788\n",
            "Epoch 1965/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2786\n",
            "Epoch 1966/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2784\n",
            "Epoch 1967/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2782\n",
            "Epoch 1968/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2780\n",
            "Epoch 1969/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2778\n",
            "Epoch 1970/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2776\n",
            "Epoch 1971/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2774\n",
            "Epoch 1972/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2772\n",
            "Epoch 1973/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2770\n",
            "Epoch 1974/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2768\n",
            "Epoch 1975/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2766\n",
            "Epoch 1976/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2764\n",
            "Epoch 1977/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2762\n",
            "Epoch 1978/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2760\n",
            "Epoch 1979/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2758\n",
            "Epoch 1980/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2756\n",
            "Epoch 1981/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2754\n",
            "Epoch 1982/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2752\n",
            "Epoch 1983/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2749\n",
            "Epoch 1984/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2747\n",
            "Epoch 1985/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2745\n",
            "Epoch 1986/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2743\n",
            "Epoch 1987/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2741\n",
            "Epoch 1988/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2739\n",
            "Epoch 1989/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2737\n",
            "Epoch 1990/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2735\n",
            "Epoch 1991/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2733\n",
            "Epoch 1992/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2731\n",
            "Epoch 1993/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2729\n",
            "Epoch 1994/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2727\n",
            "Epoch 1995/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2725\n",
            "Epoch 1996/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2723\n",
            "Epoch 1997/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2721\n",
            "Epoch 1998/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2719\n",
            "Epoch 1999/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2717\n",
            "Epoch 2000/2000\n",
            "Step 0: d_loss=16.1181, g_loss=16.1181, c_loss=0.2715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the classifier on the test dataset\n",
        "test_loss, test_accuracy = cr.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "y_pred_probs = cr.predict(test_dataset)\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred_labels = tf.argmax(y_pred_probs, axis=1).numpy()\n",
        "y_true_labels = tf.concat([y for _, y in test_dataset], axis=0).numpy()\n",
        "\n",
        "# If the labels are one-hot encoded, convert them to integers\n",
        "if y_true_labels.ndim > 1:\n",
        "    y_true_labels = tf.argmax(y_true_labels, axis=1).numpy()\n",
        "\n",
        "# Compute the confusion matrix\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(y_true_labels,y_pred_labels)\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [0,1,2,3,4,5,6,7,8,9])\n",
        "cm_display.plot()\n",
        "pt.show()\n",
        "\n",
        "confusion_matrix_per_class = confusion_matrix.astype('float') / confusion_matrix.sum(axis=1, keepdims=True) * 100\n",
        "cm_display_per_class = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_per_class,display_labels=[0,1,2,3,4,5,6,7,8,9])\n",
        "cm_display_per_class.plot(cmap=\"Blues\", values_format=\".2f\")\n",
        "pt.title(\"Confusion Matrix (Percentage Per Class)\")\n",
        "pt.show()\n",
        "\n",
        "acc_test=accuracy_score(y_true_labels,y_pred_labels)\n",
        "print(\"Accuracy Test: \", acc_test*100)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "VXb9JkeY7p1p",
        "outputId": "7a5063c0-bf9a-4255-b5a7-23f99d1e8d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8771 - loss: 0.3307\n",
            "Test Loss: 0.3307, Test Accuracy: 0.8771\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAGwCAYAAABSAee3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAX8xJREFUeJzt3XlcVPX+P/DXYYAZRmBkXwQRRREX1NS8pKWWadY1S9u89guztAzL5ZpKuZuSfu/1mqVkZWolmbfc73VBu6KmlqK4iwsqKKigwrAOzMz5/UFMjbgwDMw5w7yej8d5PJozZ3n5gXjP53POnI8giqIIIiIisktOUgcgIiKi2mMhJyIismMs5ERERHaMhZyIiMiOsZATERHZMRZyIiIiO8ZCTkREZMecpQ5gDaPRiOzsbHh4eEAQBKnjEBGRhURRRGFhIYKDg+HkVH99y7KyMpSXl1t9HFdXV6hUqjpIVHfsupBnZ2cjNDRU6hhERGSlrKwshISE1Muxy8rKEB7mjms3DFYfKzAwEBcvXpRVMbfrQu7h4QEAaPnWNChc5dOogZ/+KnUEono17PAlqSNUs+KhZlJHqMZJpZQ6QjXGMp3UEczoUYG9+K/p73l9KC8vx7UbBlxObQZPj9r3+rWFRoR1voTy8nIW8rpSNZyucFVBoZRRowouUkcgqldqD4XUEaqR4/93ToKr1BGqMQpGqSOY+/0h4ba4POruIcDdo/bnMUKel3DtupATERHVlEE0wmDF7CIGUWYfgn7HQk5ERA7BCBFG1L6SW7NvfeLXz4iIiOwYe+REROQQjDDCmsFx6/auPyzkRETkEAyiCINY++Fxa/atTxxaJyIismPskRMRkUNoqDe7sZATEZFDMEKEoQEWcg6tExER2TH2yImIyCFwaL0B6NwkG8O6piEqIBf+7iUYs+Ep/O98uOn92f1+xsB26Wb7/HIxFKPW/tXWUTFgWB5eGHUD3n56ZJxyw5IpTZCeprZ5DmZiJmudSvLAqe89UXil8hGqXi3L8VDcbTTtWWq2nSgCW98MRNYeNfouvoZmT5bYJN+fyeln166rFi+MzEFEu2L4BFRg1lstsT/ZW5Isd5JTO1mCd603AG4uFUjP9cHcnY/ec5u9F0PROzHWtEz8z5M2TFip57O3MXJ6NlYtCERcv1bIOKXCnKQMaHwqbJ6FmZjJWo0CDXj477cwaN0VPL/2KoL/Uort7wTi1jnzZ6MfX6GBlI+ylrqd7qRSG5FxWo0l05tJcv57kVs7kUwK+eLFi9GsWTOoVCp069YNv/32W72cZ++lMHz2Szf8fL75PbcpNyhws0RtWgp1tp+9aNDIPGxN8sb2H7yReU6FRZNCoCsV0G/ILZtnYSZmslbY4yVo2qsUmmZ6NA6vwMPjb8NFbcSNtD8mOso75YrjX2vQMyHXJpnuRup2utOhlMb4ZkEo9m2XRy+8itzayRLGOljkSPJC/sMPP2D8+PGYPn06Dh8+jA4dOqBfv364ceOGJHm6hGRj16jl2Ph6EqY8kQKNqsym53d2MaJldAkO7/ljSj9RFHBkjwfadLb9UCMzMVNdMhqA85sboaLECQGdKv/f0pcK+Pnv/ug+PQ9qP+vni64NubWTXNl7Oxl+v2vdmkWOJC/kCxYswIgRI/D666+jTZs2+Pzzz6FWq/H111/bPMsvl0IxZevjGPHvZ/GvPTHoHJqDJYP+AycbTvvn6W2AwhnIzzW/feF2njO8/PQ2y8FMzFSXbqW74OuOzbCsXTj2TvdF38XX4BVRORS7b64PAjqVoVkf6QqBXNpJ7uy9nQyi9YscSXqzW3l5OVJTUxEfH29a5+TkhD59+mD//v3VttfpdNDpdKbXWq22TvNsTW9p+u9zeT44m+uDLW+uQtfQbPyaGVKn5yJyJJrwCgzecAXlhU64uLURdk3yx4BV2Si47ILsA24YvP6K1BGJ7JakhTwvLw8GgwEBAQFm6wMCAnDmzJlq2yckJGDmzJm2ioerBZ64VaJCaOMCmxVy7S0FDHqg8R2fbr189bidK82Pi5mYyVoKV0ATVpnBr105co8rcXylBs4qEdpMZ6zo0sxs++R3AxDYpQwDvsuxST65tJPc2Xs7WXudm9fI60B8fDwKCgpMS1ZWVr2eL8C9CI3dypBXbLuvVegrnHDumBqdehSa1gmCiI49inAqVZqvdzATM9U1URRgLBfQcWQ+Xth0BYM3/LEAQMwHN21645tc20lu7L2djBBgsGIxSvm1ivuQ9COUr68vFAoFrl+/brb++vXrCAwMrLa9UqmEUln7u8jdXCrQtHGB6XUTTy0i/fJQUKZEQZkKo2IOYse55sgrViO0sRbjHtuPzNsa/HKpaa3PWRtrv/DFhIVZOHtUjfQjajw/IhcqtRHbV0t39yozMVNt/fYPL4T2LIV7kB4VxQLOb3JH9q8qPP31Naj9DHe9wc09SA/PUNtec5W6ne6kUhsQHPbHzbYBoTo0jypGYYEzcrNt/22aKnJrJ5K4kLu6uqJz587YuXMnnnvuOQCA0WjEzp07MXr06Do/X9uAG/j65Y2m1xN77wMAbDgRiY92PoaWfrfwbNt0eCjLcaOoEfZfDsFnvzyMCoOizrPcT8pGL2h8DHjt/Wvw8tMj46QbPhwajvw8lwfvzEzMJLNMpbcU+N9EP5TccIarhxE+kTo8/fU1hHQvffDONiR1O92pZftizP/+tOn1W1MyAQDJP/piwcQWkmQC5NdOljCKlYs1+8uRIIrSPqrmhx9+QGxsLJYuXYqHH34YCxcuxJo1a3DmzJlq187vpNVqodFo0PrduVAoVffd1paC/rlP6ghE9Wrk2QypI1TzRat7Px9CKk4q+fxdqmIss+1Xah9EL1ZgFzagoKAAnp6e9XKOqlrx68lAuHvU/opyUaER3dpeq9estSH53Qkvv/wycnNzMW3aNFy7dg0dO3bE1q1bH1jEiYiISAaFHABGjx5dL0PpREREVapuWrNmfzmSRSEnIiKqb0ZRgFGsfTG2Zt/6ZFdfPyMiIiJz7JETEZFD4NA6ERGRHTPACQYrBqKlmdLnwVjIiYjIIYhWXiMXeY2ciIiI6hp75ERE5BB4jZyIiMiOGUQnGEQrrpHL9BGtHFonIiKqB4mJiYiOjoanpyc8PT0RExODLVu2mN7v1asXBEEwW95++22Lz8MeOREROQQjBBit6L8aYVmXPCQkBB9//DFatmwJURSxcuVKDBw4EEeOHEHbtm0BACNGjMCsWbNM+6jVlk8Hy0JOREQOwdbXyAcMGGD2es6cOUhMTMSBAwdMhVytVt912m5LNIhCHvjpr3AW5DOF3ryLv0odoZpJ4d2kjkC1JMcZtOQ405gcyW2mMaobWq3W7LVSqYRSef854g0GA/7973+juLgYMTExpvWrVq3Cd999h8DAQAwYMABTp061uFfeIAo5ERHRg1h/s1vl0HpoaKjZ+unTp2PGjBl33ef48eOIiYlBWVkZ3N3dsW7dOrRp0wYA8Le//Q1hYWEIDg7GsWPHMGnSJKSnp2Pt2rUW5WIhJyIih1B5jdyKSVN+3zcrK8tsPvL79cYjIyORlpaGgoIC/Pjjj4iNjUVKSgratGmDkSNHmrZr3749goKC8MQTT+DChQto0aJFjXOxkBMREVmg6i70mnB1dUVERAQAoHPnzjh48CA++eQTLF26tNq23bpVXgI9f/48CzkREdGdjFY+a93Su9bvegyjETqd7q7vpaWlAQCCgoIsOiYLOREROYS6ukZeU/Hx8ejfvz+aNm2KwsJCJCUlYdeuXdi2bRsuXLiApKQkPP300/Dx8cGxY8cwbtw4PPbYY4iOjrboPCzkRETkEIxwsun3yG/cuIHXXnsNOTk50Gg0iI6OxrZt2/Dkk08iKysLO3bswMKFC1FcXIzQ0FAMHjwYU6ZMsTgXCzkREVE9WLZs2T3fCw0NRUpKSp2ch4WciIgcgkEUYLBiKlJr9q1PLOREROQQDFbe7Gaog5vd6gMnTSEiIrJj7JETEZFDMIpOMFpx17rRwrvWbYWFnIiIHEJDHVpnIQcwYFgeXhh1A95+emSccsOSKU2Qnmb5VHK1sf87fxz4LgC3r1Y+4i+gZQmeeO8qWvcqMG1z+bA7tv0jBJlp7nBSAMFRxXjjmzNwUdn2l0rKdmKm2mvXVYsXRuYgol0xfAIqMOutltif7C1Znipyaydmsu9Mjszhr5H3fPY2Rk7PxqoFgYjr1woZp1SYk5QBjU+FTc6vCSxH/0mZeG/jcby74QRaxGjxzchWuHbWDUBlEV82LBItHy3A6PUn8e76E4h57ToEG988KXU7MVPtqdRGZJxWY8n0ZpJluJMc24mZ7DdTTRnxx53rtVmMUv8D7kHSQr57924MGDAAwcHBEAQB69evt3mGQSPzsDXJG9t/8EbmORUWTQqBrlRAvyG3bHL+Nn3y0bp3AXzDdfBrXoan3r8CV7URmUfcAQCbZoehe+x19B6Vg8BWpfBrUYYOf70FZ6Vte+NStxMz1d6hlMb4ZkEo9m2XvhdeRY7txEz2m6mmqh4IY80iR5KmKi4uRocOHbB48WJJzu/sYkTL6BIc3uNhWieKAo7s8UCbziU2z2M0AGmbvFFe6oSwh4pQlOeMrDR3uPtUYPHgNpjd5SF8/nIULh50t2kuubUTM9k3ObYTM9lvJpL4Gnn//v3Rv39/yc7v6W2AwhnIzzVvhtt5zgiNuPtD7etDzhk3LBncFnqdE1zVBrz2+VkEtCzF5d975Ts+aYKnP8hEcJsSHF7riy9fjcL4rcfgG26bjHJpJ2ZqGOTYTsxkv5ksYf2z1uXZI7erm910Op3ZrDFarVbCNHXHr3kZxvznOMoKFTi+xQdrJrTAW6tPQ/z9gky3v91A1xfzAABN2mbi/C8aHPy3P/pPzJIwNRGRfamr+cjlRp4fL+4hISEBGo3GtISGhlp1PO0tBQx6oLGf3my9l68et3Nt9xnH2VWEbzMdQtqXoP/ELARFlWDv8gB4+lfePOIfUWq2vX9EKfKzXW2WTy7txEwNgxzbiZnsN5Mlqnrk1ixyJM9U9xAfH4+CggLTkpVlXY9UX+GEc8fU6NSj0LROEER07FGEU6nSfZVCNAKGcid4hejgGVCO3Aw3s/fzLqrg1cR2w1hybCdmsl9ybCdmst9MZGdD60qlEkqlsk6PufYLX0xYmIWzR9VIP6LG8yNyoVIbsX21be7w3TI/FJE989G4iQ66IgXSNvoi44Anhq88A0EAHhuZg+SFTRAUVYLgNsVI/ckPNy644dUl52ySr4rU7cRMtadSGxAcVmZ6HRCqQ/OoYhQWOCM3u27/f6opObYTM9lvppqy/oEw8uz72lUhrw8pG72g8THgtfevwctPj4yTbvhwaDjy81xscv6im85Y8/cW0Oa6QOVhQFDrEgxfeQatHq28/v/o8GvQ6wRs/qgpSvKdERRVgje/PQ2fMNveWCJ1OzFT7bVsX4z53582vX5rSiYAIPlHXyyY2EKSTHJsJ2ay30w1ZRQFGK2YwcyafeuTIIrSPTy2qKgI58+fBwB06tQJCxYsQO/eveHt7Y2mTZs+cH+tVguNRoNeGAhnQT6/RPMu/ip1hGomhXeTOgLVkpNKJXWEaoxlZQ/eiKgG9GIFdmEDCgoK4OnpWS/nqKoV8w8+Cjf32vdfS4v0mNh1T71mrQ1Je+SHDh1C7969Ta/Hjx8PAIiNjcWKFSskSkVERA2R0cqhdbk+EEbSQt6rVy9IOCBAREQOxPrZz+RZyOWZioiIiGrE4W92IyIix2CAAIMVD3WxZt/6xEJOREQOgUPrREREJDvskRMRkUMwwLrhcUPdRalTLOREROQQGurQOgs5ERE5hIY6jak8UxEREVGNsEdOREQOQbRyPnKRXz8jIiKSDofWiYiISHbYI68HcpxpbFt2mtQRqukX3FHqCHaBM43VDGeJowdpqNOYspATEZFDMFg5+5k1+9YneaYiIiKiGmGPnIiIHAKH1omIiOyYEU4wWjEQbc2+9UmeqYiIiKhG2CMnIiKHYBAFGKwYHrdm3/rEQk5ERA6hoV4j59A6ERE5BPH32c9qu4gWPtktMTER0dHR8PT0hKenJ2JiYrBlyxbT+2VlZYiLi4OPjw/c3d0xePBgXL9+3eJ/Fws5ERFRPQgJCcHHH3+M1NRUHDp0CI8//jgGDhyIkydPAgDGjRuHTZs24d///jdSUlKQnZ2NQYMGWXweDq0TEZFDMECAwYqJT6r21Wq1ZuuVSiWUSmW17QcMGGD2es6cOUhMTMSBAwcQEhKCZcuWISkpCY8//jgAYPny5YiKisKBAwfwl7/8pca52CMnIiKHYBT/uE5eu6XyOKGhodBoNKYlISHhgec2GAxYvXo1iouLERMTg9TUVFRUVKBPnz6mbVq3bo2mTZti//79Fv272CMnIiKyQFZWFjw9PU2v79Ybr3L8+HHExMSgrKwM7u7uWLduHdq0aYO0tDS4urqicePGZtsHBATg2rVrFuVhIQcwYFgeXhh1A95+emSccsOSKU2QnqZ22EybVvrgP9/44nqWKwAgLLIMQ8ddQ9fHCwEA2Zdc8eWsYJz8zR0V5QI699Yi7qOr8PLT2yTfn/Fnx0x1oV1XLV4YmYOIdsXwCajArLdaYn+ytyRZ7iSndpJzppqoumnNmv0BmG5eq4nIyEikpaWhoKAAP/74I2JjY5GSklLrDHfj8EPrPZ+9jZHTs7FqQSDi+rVCxikV5iRlQONT4bCZ/IIqMPyDbHy2NR2fbjmLDt0LMeP1cFxKV6GsxAkfDGkBQQDm/fs8Fmw4B325E6bFhsNotEk8E6nbiZkaTiaV2oiM02osmd5MkvPfi9zaSa6ZasoIwerFUq6uroiIiEDnzp2RkJCADh064JNPPkFgYCDKy8uRn59vtv3169cRGBho0TkkLeQJCQno2rUrPDw84O/vj+eeew7p6ek2zTBoZB62Jnlj+w/eyDynwqJJIdCVCug35JZNc8gp01/6avHwE4Vo0rwcIS10eH3yNagaGXEmVY2TvzXC9SxX/H1hJsKjyhAeVYb3P7mMc0fVSNvrbpN8VaRuJ2ZqOJkOpTTGNwtCsW+7PHrhVeTWTnLNZE+MRiN0Oh06d+4MFxcX7Ny50/Reeno6MjMzERMTY9ExJS3kKSkpiIuLw4EDB5CcnIyKigr07dsXxcXFNjm/s4sRLaNLcHiPh2mdKAo4sscDbTqX2CSD3DMZDMCu9Y2hK3FCVJdiVJQLgAC4uIqmbVyUIgQn4ORvtivkcmsnZrLvTHIkx3aSYyZLVD3ZzZrFEvHx8di9ezcuXbqE48ePIz4+Hrt27cLQoUOh0WjwxhtvYPz48fjf//6H1NRUvP7664iJibHojnVA4mvkW7duNXu9YsUK+Pv7IzU1FY899li9n9/T2wCFM5Cfa94Mt/OcERqhq/fzyznTxdMqjB3QEuU6J7g1MmLasosIa6WDxkcPldqIZXOC8frkbAACls0JgtEg4NYN2/06yaWdmKlhZJIjObaTHDNZoq6ukdfUjRs38NprryEnJwcajQbR0dHYtm0bnnzySQDAv/71Lzg5OWHw4MHQ6XTo168flixZYnEuWd3sVlBQAADw9r778JZOp4NO98cvy53f5aO6E9JChyXJ6SgpVGDP5sb4x5gw/N/acwhrpcOUpZfwaXwINizzheAE9H7uNiLal0Bw+DsuiIj+sGzZsvu+r1KpsHjxYixevNiq88imkBuNRowdOxbdu3dHu3bt7rpNQkICZs6cWWfn1N5SwKAHGt9xt7WXrx63c6VpGrlkcnEV0SS8HADQMroU6WlqrP/KD2PmX0HnXoVYsf80Cm4qoHAG3DUGvNKhLYKa2u4TuVzaiZkaRiY5kmM7yTGTJYyw8lnrVjxMpj7Jpg8VFxeHEydOYPXq1ffcJj4+HgUFBaYlKyvLqnPqK5xw7pganXoUmtYJgoiOPYpwKlWar1LIMRMAiCJQUW7+66LxMcBdY0DaXnfk5znjL31tN0Iix3ZiJvvNJEdybCc5ZrKEaOUd66JMC7ksPkKNHj0amzdvxu7duxESEnLP7e71GDxrrP3CFxMWZuHsUTXSj6jx/IhcqNRGbF8t3d2rUmf6em4Quj6uhV+TCpQWOeF/67xwbJ875iRdAABsW+2Npi3LoPHR43RqIyROa4LnR+ba/BqZ1O3ETA0nk0ptQHBYmel1QKgOzaOKUVjgjNzsuv2bYwm5tZNcM9VUQ539TNJCLooi3n33Xaxbtw67du1CeHi4zTOkbPSCxseA196/Bi8/PTJOuuHDoeHIz3OxeRa5ZMrPc8b/vReGWzecofYwIDyqDHOSLqBzzyIAwJULSixPCEJhvgIBoeUY8t51DBqZa5NsfyZ1OzFTw8nUsn0x5n9/2vT6rSmZAIDkH32xYGILSTIB8msnuWZydIIoiuKDN6sf77zzDpKSkrBhwwZERkaa1ms0Gri5uT1wf61WC41Gg14YCGeBv0T3sy07TeoI1fQL7ih1BGpAnFQqqSNUYywre/BGDk4vVmAXNqCgoKDGT0uzVFWteD75dbg0cq31cSqKy7HuyeX1mrU2JO2RJyYmAgB69epltn758uUYNmyY7QMREVGDxaH1eiDhYAAREVGDIIub3YiIiOpbbZ+X/uf95YiFnIiIHEJDHVqXzffIiYiIyHLskRMRkUNoqD1yFnIiInIIDbWQc2idiIjIjrFHTkREDqGh9shZyImIyCGIsO4rZHJ98gkLOREROYSG2iPnNXIiIiI7xh45ERE5hIbaI2chdxBynGmsz4lCqSNUs6Odh9QRqJY40xg9SEMt5BxaJyIismPskRMRkUNoqD1yFnIiInIIoihAtKIYW7NvfeLQOhERkR1jj5yIiBwC5yMnIiKyYw31GjmH1omIiOwYe+REROQQGurNbizkRETkEBrq0DoLOREROYSG2iPnNXIiIiI7xh45ERE5BNHKoXW59shZyAEMGJaHF0bdgLefHhmn3LBkShOkp6mZSUaZrqx2wZUfXFCaXTmI5B5hRPjbOvg+akBFAXBhsRK39jmjLEeAi5cI/8f1aPGuDs4SzIHCnx0zMZM8iQBE0br95cjhh9Z7PnsbI6dnY9WCQMT1a4WMUyrMScqAxqeCmWSUSRloRMQ4HbqtKcbDPxTD62E9jr7rhqLzTtDdcILuhoCWE8rwl3XFaDunDDd/ccapaSqbZPszqduJmZjJETM5OkkLeWJiIqKjo+Hp6QlPT0/ExMRgy5YtNs0waGQetiZ5Y/sP3sg8p8KiSSHQlQroN+SWTXMw0/359TLA9zED1GEiGjUTETGmHAo1UHBUAfeWRnRYWAa/Xgaom4rw7mZAi/d0yN3lDKPeJvFMpG4nZmImR8xUU1VPdrNmkSNJC3lISAg+/vhjpKam4tChQ3j88ccxcOBAnDx50ibnd3YxomV0CQ7v+WP8VRQFHNnjgTadS2ySgZksJxqAa/91hqEU0HQ03HUbfaEAZ3cRTja8eCS3dmImZnKETJaoumvdmkWOJL1GPmDAALPXc+bMQWJiIg4cOIC2bdtW216n00Gn05lea7Vaq87v6W2AwhnIzzVvhtt5zgiN0N1jr/rFTPdWdNYJB4eqYSwHFGqgwyelcG9hrLZd+W0BF5e6oskLth3qk0s7MRMzOVImktE1coPBgNWrV6O4uBgxMTF33SYhIQEajca0hIaG2jglSUkdbkS3n4rRNakEIS+V4+SHKhRdMP8V1hcBae+4oVELI5q/Uy5RUiKSo6oHwlizWCIhIQFdu3aFh4cH/P398dxzzyE9Pd1sm169ekEQBLPl7bfftug8khfy48ePw93dHUqlEm+//TbWrVuHNm3a3HXb+Ph4FBQUmJasrCyrzq29pYBBDzT2M7+Q6uWrx+1caQYrmOnenFwAdVMRnm2NiBhXDo9II7K+czG9ry8GjrylhnMjEdGflMLJ5T4HqwdyaSdmYiZHymQJUbR+sURKSgri4uJw4MABJCcno6KiAn379kVxcbHZdiNGjEBOTo5pmT9/vkXnkbyQR0ZGIi0tDb/++itGjRqF2NhYnDp16q7bKpVK041xVYs19BVOOHdMjU49Ck3rBEFExx5FOJUqzVcpmKnmRCNgLK/8hKwvAo6MVENwEdHh01IolLbPI8d2YiZmauiZ5Gzr1q0YNmwY2rZtiw4dOmDFihXIzMxEamqq2XZqtRqBgYGmxdLaJvlHKFdXV0RERAAAOnfujIMHD+KTTz7B0qVLbXL+tV/4YsLCLJw9qkb6ETWeH5ELldqI7au9bXJ+ZqqZ8/9yhc+jBqiCjDAUC7j2H2fcPqhAp6Wl0BcBh0eqYSwFoj8pg75YgP73D7yuXiIEhU0iApC+nZiJmRwxU03V1SNa77w/S6lUQql8cO+hoKAAAODtbd5Wq1atwnfffYfAwEAMGDAAU6dOhVpd8w9GkhfyOxmNRrMb2upbykYvaHwMeO39a/Dy0yPjpBs+HBqO/Dwbj8sy032V3xJw8gMVdLkCnD1EeLQyotPSUvg8YsCt3xTQHqus1vuedjfbr/u2Irg1sd1jHKRuJ2ZiJkfMVFN1VcjvvD9r+vTpmDFjxn33NRqNGDt2LLp374527dqZ1v/tb39DWFgYgoODcezYMUyaNAnp6elYu3ZtjXMJomjNc26sEx8fj/79+6Np06YoLCxEUlIS5s2bh23btuHJJ5984P5arRYajQa9MBDOgvx/ichcnxOFD97Ixna0k+BRcEQOTC9WYBc2oKCgwOrLpfdSVSsikyZDoa79dTdDiQ7pf/sYWVlZZllr0iMfNWoUtmzZgr179yIkJOSe2/3888944okncP78ebRo0aJGuSTtkd+4cQOvvfYacnJyoNFoEB0dXeMiTkREJAVL79EaPXo0Nm/ejN27d9+3iANAt27dAMB+CvmyZcukPD0RETmQ2tx5fuf+lm0v4t1338W6deuwa9cuhIeHP3CftLQ0AEBQUFCNzyO7a+RERET1obKQW3ON3LLt4+LikJSUhA0bNsDDwwPXrl0DAGg0Gri5ueHChQtISkrC008/DR8fHxw7dgzjxo3DY489hujo6Bqfh4WciIioHiQmJgKofOjLny1fvhzDhg2Dq6srduzYgYULF6K4uBihoaEYPHgwpkyZYtF5WMiJiMgh1NVd6zXf/v5d+NDQUKSkpNQ6TxUWciIicggirJtTnPORExERUZ1jj5yIiByCrYfWbYWFnIiIHEMDHVtnISciIsdgZY8cMu2R8xo5ERGRHWOPnIiIHIKtn+xmKyzkRETkEHizG1Edk+NMY/Mu/ip1hGrio3pKHaEaY1mZ1BGI6Hcs5ERE5BhEwbob1tgjJyIikk5DvUbOu9aJiIjsGHvkRETkGPhAGCIiIvvl0Hetb9y4scYHfPbZZ2sdhoiIiCxTo0L+3HPP1ehggiDAYDBYk4eIiKj+yHR43Bo1KuRGo7G+cxAREdWrhjq0btVd62V8KAQREdkLsQ4WGbK4kBsMBsyePRtNmjSBu7s7MjIyAABTp07FsmXL6jwgERER3ZvFhXzOnDlYsWIF5s+fD1dXV9P6du3a4auvvqrTcERERHVHqINFfiwu5N988w2++OILDB06FAqFwrS+Q4cOOHPmTJ2GIyIiqjMNdGjd4u+RX716FREREdXWG41GVFRU1EkoWxswLA8vjLoBbz89Mk65YcmUJkhPUzMTM93X/u/8ceC7ANy+qgQABLQswRPvXUXrXgWmbS4fdse2f4QgM80dTgogOKoYb3xzBi4q2/1FaNdVixdG5iCiXTF8Aiow662W2J/sbbPz3wt/n5iJ6obFPfI2bdpgz5491db/+OOP6NSpU52EsqWez97GyOnZWLUgEHH9WiHjlApzkjKg8ZHuQwkz2UcmTWA5+k/KxHsbj+PdDSfQIkaLb0a2wrWzbgAqi/iyYZFo+WgBRq8/iXfXn0DMa9ch2Hh0TqU2IuO0GkumN7Ptie9D6p8dMzWsTDXWQHvkFhfyadOmYfTo0Zg3bx6MRiPWrl2LESNGYM6cOZg2bVqtg3z88ccQBAFjx46t9TFqY9DIPGxN8sb2H7yReU6FRZNCoCsV0G/ILZvmYCb7y9SmTz5a9y6Ab7gOfs3L8NT7V+CqNiLziDsAYNPsMHSPvY7eo3IQ2KoUfi3K0OGvt+CstO1fg0MpjfHNglDs2y59L7yK1D87ZmpYmWqsavYzaxYZsriQDxw4EJs2bcKOHTvQqFEjTJs2DadPn8amTZvw5JNP1irEwYMHsXTpUkRHR9dq/9pydjGiZXQJDu/5Y15sURRwZI8H2nQusWkWZrLvTEYDkLbJG+WlTgh7qAhFec7ISnOHu08FFg9ug9ldHsLnL0fh4kF3m2eTG7n97JjJvjNRLZ+1/uijjyI5OblOAhQVFWHo0KH48ssv8dFHH913W51OB51OZ3qt1WqtOrentwEKZyA/17wZbuc5IzRCd4+96hcz2VemnDNuWDK4LfQ6J7iqDXjt87MIaFmKy7/3ynd80gRPf5CJ4DYlOLzWF1++GoXxW4/BN1yadpMDufzsmKlhZLIEpzG9w6FDh/Dtt9/i22+/RWpqaq0DxMXF4ZlnnkGfPn0euG1CQgI0Go1pCQ0NrfV5ieqCX/MyjPnPccStO4G/vHoDaya0wPVzbhB/fxhit7/dQNcX89CkbQkGTM2EX3gZDv7bX9rQRI6qgV4jt7hHfuXKFQwZMgS//PILGjduDADIz8/HI488gtWrVyMkJKTGx1q9ejUOHz6MgwcP1mj7+Ph4jB8/3vRaq9VaVcy1txQw6IHGfnqz9V6+etzOlWZiOGayr0zOriJ8m1X2RELal+DKsUbYuzwAvUflAAD8I0rNtvePKEV+tmu14zgSufzsmKlhZKJa9MjffPNNVFRU4PTp07h16xZu3bqF06dPw2g04s0336zxcbKysjBmzBisWrUKKpWqRvsolUp4enqaLdbQVzjh3DE1OvUoNK0TBBEdexThVKo0X6VgJvvNBACiETCUO8ErRAfPgHLkZriZvZ93UQWvJvIfgqxPcvzZMZP9ZrJIA73ZzeKPUCkpKdi3bx8iIyNN6yIjI/Hpp5/i0UcfrfFxUlNTcePGDTz00EOmdQaDAbt378Znn30GnU5n9sCZ+rL2C19MWJiFs0fVSD+ixvMjcqFSG7F9tXR3+DKTfWTaMj8UkT3z0biJDroiBdI2+iLjgCeGrzwDQQAeG5mD5IVNEBRVguA2xUj9yQ83Lrjh1SXnbJKvikptQHDYH/MiBITq0DyqGIUFzsjNVto0SxWpf3bM1LAy1ZQgVi7W7C9HFhfy0NDQuz74xWAwIDg4uMbHeeKJJ3D8+HGzda+//jpat26NSZMm2aSIA0DKRi9ofAx47f1r8PLTI+OkGz4cGo78PBebnJ+Z7DdT0U1nrPl7C2hzXaDyMCCodQmGrzyDVo9W3oT56PBr0OsEbP6oKUrynREUVYI3vz0NnzDb9shbti/G/O9Pm16/NSUTAJD8oy8WTGxh0yxVpP7ZMVPDylRj1l7nlmkhF0TRsvvwNmzYgLlz52Lx4sXo0qULgMob3959911MmjSpxnOX302vXr3QsWNHLFy4sEbba7VaaDQa9MJAOAt28EtEsjfv4q9SR6gmPqqn1BGqMXLmQ6ojerECu7ABBQUFVl8uvZeqWhG6cBac3Gp2KfdujKVlyBo7rV6z1kaNeuReXl4Q/vQ4quLiYnTr1g3OzpW76/V6ODs7Y/jw4VYVciIionpj7XVue75GXtMesrV27dplk/MQEZEDaqBD6zUq5LGxsfWdg4iIiGrBqi/+lZWVoby83GydnK4bEBERmTTQHrnF3yMvLi7G6NGj4e/vj0aNGsHLy8tsISIikqUG+mQ3iwv5xIkT8fPPPyMxMRFKpRJfffUVZs6cieDgYHzzzTf1kZGIiMjuJCQkoGvXrvDw8IC/vz+ee+45pKenm21TVlaGuLg4+Pj4wN3dHYMHD8b169ctOo/FhXzTpk1YsmQJBg8eDGdnZzz66KOYMmUK5s6di1WrVll6OCIiItuw8ZPdUlJSEBcXhwMHDiA5ORkVFRXo27cviouLTduMGzcOmzZtwr///W+kpKQgOzsbgwYNsug8Fl8jv3XrFpo3bw6g8nr4rVuVc9D26NEDo0aNsvRwRERENlFXT3a7c+ZNpVIJpbL6UxK3bt1q9nrFihXw9/dHamoqHnvsMRQUFGDZsmVISkrC448/DgBYvnw5oqKicODAAfzlL3+pUS6Le+TNmzfHxYsXAQCtW7fGmjVrAFT21KsmUSEiImqoQkNDzWbiTEhIqNF+BQUFAABv78rH2aampqKiosJs9s/WrVujadOm2L9/f43zWNwjf/3113H06FH07NkTkydPxoABA/DZZ5+hoqICCxYssPRwREREtlFHd61nZWWZfUPrbr3xOxmNRowdOxbdu3dHu3btAADXrl2Dq6trtU5wQEAArl27VuNYFhfycePGmf67T58+OHPmDFJTUxEREYHo6GhLD0dERGRXajP7ZlxcHE6cOIG9e/fWeR6rJ5ANCwtDWFhYXWQhIiKqNwKsvEZey/1Gjx6NzZs3Y/fu3QgJCTGtDwwMRHl5OfLz88165devX0dgYGCNj1+jQr5o0aIaH/C9996r8bZEREQNlSiKePfdd7Fu3Trs2rUL4eHhZu937twZLi4u2LlzJwYPHgwASE9PR2ZmJmJiYmp8nhoV8n/96181OpggCCzkZNfkONNYp/0lUkeoJrWTxffJEknPxpOmxMXFISkpCRs2bICHh4fpurdGo4Gbmxs0Gg3eeOMNjB8/Ht7e3vD09MS7776LmJiYGt+xDtSwkFfdpU5ERGS3bPyI1sTERACVU3T/2fLlyzFs2DAAlR1lJycnDB48GDqdDv369cOSJUssOo/V18iJiIioOlF8cOVXqVRYvHgxFi9eXOvzsJATEZFjaKCTprCQExGRQ6irJ7vJDe9YISIismPskRMRkWNooEPrteqR79mzB6+++ipiYmJw9epVAMC3335bL0+sISIiqhOcj7zSTz/9hH79+sHNzQ1HjhyBTqcDUPkw+Llz59Z5QCIiIro3iwv5Rx99hM8//xxffvklXFxcTOu7d++Ow4cP12k4IiKiulJ1s5s1ixxZfI08PT0djz32WLX1Go0G+fn5dZGJiIio7tn4yW62YnGPPDAwEOfPn6+2fu/evWjevHmdhCIiIqpzvEZeacSIERgzZgx+/fVXCIKA7OxsrFq1ChMmTMCoUaPqIyMRERHdg8VD65MnT4bRaMQTTzyBkpISPPbYY1AqlZgwYQLefffd+shY7wYMy8MLo27A20+PjFNuWDKlCdLT1MzETBZr11WLF0bmIKJdMXwCKjDrrZbYn+xts/PnrgFyfxSgy6587dYcCBopQtPj9/d/Am5tEVByBjAWC+iw2whnD5vFMyO3nx0z2XemmuADYX4nCAI+/PBD3Lp1CydOnMCBAweQm5uL2bNn10e+etfz2dsYOT0bqxYEIq5fK2ScUmFOUgY0PhXMxEwWU6mNyDitxpLpzSQ5v0sA0ORdEVGrKhePh4EL4wSUXqh831gGaB4RETRc2r9IcvzZMZP9ZqoxDq2bc3V1RZs2bfDwww/D3d29VseYMWMGBEEwW1q3bl3bSLUyaGQetiZ5Y/sP3sg8p8KiSSHQlQroN+SWTXMwU8PIdCilMb5ZEIp9223XC/+zxj0BzaOAKqxyaTJahJMaKD5W+X7AUCBwONAoWpJ4JnL82TGT/WZydBYPrffu3RuCcO87937++WeLjte2bVvs2LHjj0DOtnvYnLOLES2jS7D6M3/TOlEUcGSPB9p0lmYOaGay30xyIxqA28mAsVT6wv1ncvzZMZP9ZrKItV8hk2mP3OKq2bFjR7PXFRUVSEtLw4kTJxAbG2t5AGdnBAYG1mhbnU5negANAGi1WovP92ee3gYonIH8XPNmuJ3njNAI3T32ql/MZL+Z5KL0HHAmVoCxHFC4AS3+KcKthdSp/iDHnx0z2W8mizTQR7RaXMj/9a9/3XX9jBkzUFRUZHGAc+fOITg4GCqVCjExMUhISEDTpk3vum1CQgJmzpxp8TmIHImyGRC1WoShCMjfIeDSNAGtvpJXMSeiulNns5+9+uqr+Prrry3ap1u3blixYgW2bt2KxMREXLx4EY8++igKCwvvun18fDwKCgpMS1ZWllWZtbcUMOiBxn56s/VevnrczpVmPhlmst9McuHkAqiaAo3aAE3eE+HWCrjxvXweZCHHnx0z2W8mi/Bmt/vbv38/VCqVRfv0798fL774IqKjo9GvXz/897//RX5+PtasWXPX7ZVKJTw9Pc0Wa+grnHDumBqdevzxwUEQRHTsUYRTqdJ8lYKZ7DeTbImAWC51iD/I8WfHTPabyRJ8ROvvBg0aZPZaFEXk5OTg0KFDmDp1qlVhGjdujFatWt31yXH1Ze0XvpiwMAtnj6qRfkSN50fkQqU2Yvtqae46Zib7zqRSGxAcVmZ6HRCqQ/OoYhQWOCM3W1nv57+6SIBndxGuQYCxuPI744WHgJZLKv8CVeQBFTcBXWbl9qXnAEUjwDUQcNbUezwTOf7smMl+Mzk6iwu5RmP+f7uTkxMiIyMxa9Ys9O3b16owRUVFuHDhAv7f//t/Vh3HEikbvaDxMeC196/By0+PjJNu+HBoOPLzXB68MzMx0x1ati/G/O9Pm16/NaWyYib/6IsFE+v/InXFLeDSVAEVeYDCHXBrWVnEPf9S+X7ujwJylv4xzH72jcpBubCZRvg+W+/xTOT4s2Mm+83k6ARRFGs8WGAwGPDLL7+gffv28PLysvrkEyZMwIABAxAWFobs7GxMnz4daWlpOHXqFPz8/B64v1arhUajQS8MhLPAXyKynpOFl4dsodN++X2tJ7VTnV2VIwenFyuwCxtQUFBg9eXSe6mqFS3i50Jhxf/jhrIyXEj4oF6z1oZFPXKFQoG+ffvi9OnTdVLIr1y5giFDhuDmzZvw8/NDjx49cODAgRoVcSIiIks01Ee0Wjy03q5dO2RkZCA8PNzqk69evdrqYxARETkyi8fHPvroI0yYMAGbN29GTk4OtFqt2UJERCRbDeyrZ4AFPfJZs2bh73//O55++mkAwLPPPmv2qFZRFCEIAgwGQ92nJCIispajP9lt5syZePvtt/G///2vPvMQERGRBWpcyKtubu/Zs2e9hSEiIqovvNkNuO+sZ0RERLLm6EPrANCqVasHFvNbtzgnLRERka1YVMhnzpxZ7cluRERE9oBD6wBeeeUV+Pv7P3hDIiIiuWmgQ+s1/h45r48TERHJj8V3rRMREdmlBtojr3EhNxqN9ZmDiIioXvEaOZEDMJaVPXgjG5PjTGPnv+skdYRqIl49InWEauQ4m54cf8dtpoH2yOX3F4KIiIhqjIWciIgcgzUTptSiN797924MGDAAwcHBEAQB69evN3t/2LBhEATBbHnqqacs/mexkBMRkUOoukZuzWKJ4uJidOjQAYsXL77nNk899RRycnJMy/fff2/xv4vXyImIiOpB//790b9///tuo1QqERgYaNV52CMnIiLHUEdD61qt1mzR6XS1jrRr1y74+/sjMjISo0aNws2bNy0+Bgs5ERE5hLoaWg8NDYVGozEtCQkJtcrz1FNP4ZtvvsHOnTsxb948pKSkoH///jAYDBYdh0PrREREFsjKyoKnp6fptVKprNVxXnnlFdN/t2/fHtHR0WjRogV27dqFJ554osbHYY+ciIgcQx0NrXt6epottS3kd2revDl8fX1x/vx5i/Zjj5yIiByDzB8Ic+XKFdy8eRNBQUEW7cdCTkREVA+KiorMetcXL15EWloavL294e3tjZkzZ2Lw4MEIDAzEhQsXMHHiRERERKBfv34WnYeFnIiIHILw+2LN/pY4dOgQevfubXo9fvx4AEBsbCwSExNx7NgxrFy5Evn5+QgODkbfvn0xe/Zsi4fqWciJiMgx2HhovVevXvedOXTbtm1WhPkDCzmAAcPy8MKoG/D20yPjlBuWTGmC9DQ1MzETM9UBr43X0OhgAVxzymB0dUJZy0a4+XIwKoL/mFDEb1km1CcLobhdAVGlQGnLRrj5ivk2tiCnn127rlq8MDIHEe2K4RNQgVlvtcT+ZG9JstxJTu1kiYY6+5nD37Xe89nbGDk9G6sWBCKuXytknFJhTlIGND4VzMRMzFQHVKeLUPCkL67MaIXsSS0g6EUEzzsPoeyP78rqwtW4PjIMmfOjkD2xBQSxchsYbfeXU+p2upNKbUTGaTWWTG8myfnvRW7tRDIo5FevXsWrr74KHx8fuLm5oX379jh06JDNzj9oZB62Jnlj+w/eyDynwqJJIdCVCug35JbNMjATMzXkTDmTIlD4mA/KQ9xQHqbG9beawuVmBZSXSk3baB/3RVlrd+j9lNCFq3HzxWC43KyAc265TTIC0rfTnQ6lNMY3C0Kxb7s8euFV5NZOFrHxpCm2Imkhv337Nrp37w4XFxds2bIFp06dwj//+U94eXnZ5PzOLka0jC7B4T0epnWiKODIHg+06VxikwzMxEyOlklRYgQAGBsp7vq+UGaA5+6bqPBzhd7HxSaZ5NhOctQg2qmBFXFA4mvk8+bNQ2hoKJYvX25aFx4efs/tdTqd2TNttVqtVef39DZA4Qzk55o3w+08Z4RG1P7ZuczETMx0D0YRvt9dQWmrRigPdTN7yzM5F76rs+GkM6I8SImrkyMAZ9v0NWTXTjLFdpInSXvkGzduRJcuXfDiiy/C398fnTp1wpdffnnP7RMSEsyebxsaGmrDtERkLb+VV+B6pQzX4ppVe6+ouzey5kTiypSWqAhUIvDTixDKjbYPSQ2WracxtRVJC3lGRgYSExPRsmVLbNu2DaNGjcJ7772HlStX3nX7+Ph4FBQUmJasrCyrzq+9pYBBDzT205ut9/LV43auNIMVzMRMDTWT78osqI8U4OoHETD4uFZ736hWoCJQhbLW7sgZEw7XHB0aHcq3STY5tZOc2X078Rp53TMajXjooYcwd+5cdOrUCSNHjsSIESPw+eef33V7pVJZ7Rm31tBXOOHcMTU69Sg0rRMEER17FOFUqjRfpWAmZmpwmUQRviuz4H6oANkfREDvX4OHXYiV+wl62/zllEU72QG2kzxJ+hEqKCgIbdq0MVsXFRWFn376yWYZ1n7hiwkLs3D2qBrpR9R4fkQuVGojtq+W7k5RZmKmhpTJb8UVuO+/jZxx4TCqFFDkV35NyahWQHR1gvMNHTwO3EZJe08YPJzhfKscXpuuQ3R1QkkH6z6sW0LqdrqTSm1AcFiZ6XVAqA7No4pRWOCM3Oy6maSjNuTWTpZoqN8jl7SQd+/eHenp6Wbrzp49i7CwMJtlSNnoBY2PAa+9fw1efnpknHTDh0PDkZ9nm7tlmYmZGnomzc48AEDIHPMZna6PbIrCx3wgujhBlV4MzdZcKIoN0GucUdbaHVemtYJBY7t2k7qd7tSyfTHmf3/a9PqtKZkAgOQffbFgYgtJMgHyayeLyHzSlNoSxPs9P66eHTx4EI888ghmzpyJl156Cb/99htGjBiBL774AkOHDn3g/lqtFhqNBr0wEM6CHfwSETUQ57/rJHWEaiJePSJ1hGqcVLZ9Ml1NGMvKHryRDenFCuzCBhQUFFh9ufReqmpF+zfmQuFa+5+JobwMx5d9UK9Za0PSa+Rdu3bFunXr8P3336Ndu3aYPXs2Fi5cWKMiTkREZImGete65LcZ/vWvf8Vf//pXqWMQEVFD10CH1iUv5ERERDbRQAu55M9aJyIiotpjj5yIiBwCv35GRERkzzi0TkRERHLDHjkRETkEQRQhWPHoFGv2rU8s5ERE5Bg4tE5ERERywx45ERE5BN61TkREZM84tE5ERERywx45EVlMjjONjTybIXWEar5o1VzqCPQnHFonIiKyZw10aJ2FnIiIHEJD7ZHzGjkREZEdY4+ciIgcA4fWiYiI7Jtch8etwaF1IiIiO8YeOREROQZRrFys2V+GWMiJiMgh8K51IiIikh32yImIyDHwrnUiIiL7JRgrF2v2lyMOrRMREdWD3bt3Y8CAAQgODoYgCFi/fr3Z+6IoYtq0aQgKCoKbmxv69OmDc+fOWXweFnIAA4blYeWvp7Ap4xg+2XwOkR1LpI7ETMzETPXoVJIHfhzQBMs7NcPyTs2w/qVgZKa4VdtOFIEtbwTii1bNcSlZbbN8f8afXR0S62CxQHFxMTp06IDFixff9f358+dj0aJF+Pzzz/Hrr7+iUaNG6NevH8rKyiw6j8MX8p7P3sbI6dlYtSAQcf1aIeOUCnOSMqDxqWAmZmKmBpqpUaABD//9Fgatu4Ln115F8F9Ksf2dQNw652K23fEVGkCwSaS7krqd7CVTTVXdtW7NYon+/fvjo48+wvPPP1/tPVEUsXDhQkyZMgUDBw5EdHQ0vvnmG2RnZ1fruT+IpIW8WbNmEASh2hIXF2ezDING5mFrkje2/+CNzHMqLJoUAl2pgH5DbtksAzMxEzPZNlPY4yVo2qsUmmZ6NA6vwMPjb8NFbcSNNJVpm7xTrjj+tQY9E3JtkulupG4ne8lUY1XfI7dmAaDVas0WnU5ncZSLFy/i2rVr6NOnj2mdRqNBt27dsH//fouOJWkhP3jwIHJyckxLcnIyAODFF1+0yfmdXYxoGV2Cw3s8TOtEUcCRPR5o01maoSJmYiZmsi2jATi/uREqSpwQ0KlySFNfKuDnv/uj+/Q8qP0MNs8EyK+d5JpJCqGhodBoNKYlISHB4mNcu3YNABAQEGC2PiAgwPReTUl617qfn5/Z648//hgtWrRAz54977q9Tqcz++Sj1WqtOr+ntwEKZyA/17wZbuc5IzTC8k9YdYGZmImZbONWugvWv9wEBp0AF7URfRdfg1dE5fDwvrk+COhUhmZ9pCtOcmknuWeyRF09ECYrKwuenp6m9Uql0spk1pHNNfLy8nJ89913GD58OATh7helEhISzD4FhYaG2jglETUUmvAKDN5wBc/9+yraDNFi1yR/3D7vgks71cg+4IZHPrwpdUSqa3V0s5unp6fZUptCHhgYCAC4fv262frr16+b3qsp2RTy9evXIz8/H8OGDbvnNvHx8SgoKDAtWVlZVp1Te0sBgx5o7Kc3W+/lq8ftXGkGK5iJmZjJNhSugCZMD7925Xh4wm34tNbh+EoNsg+4QZvpjBVdmuHLqHB8GRUOAEh+NwCbXg2yWT65tJPcM9mr8PBwBAYGYufOnaZ1Wq0Wv/76K2JiYiw6lmwK+bJly9C/f38EBwffcxulUlntk5A19BVOOHdMjU49Ck3rBEFExx5FOJUqzVdNmImZmEkaoijAWC6g48h8vLDpCgZv+GMBgJgPbtr0xjc5tpMcM1nC1netFxUVIS0tDWlpaQAqb3BLS0tDZmYmBEHA2LFj8dFHH2Hjxo04fvw4XnvtNQQHB+O5556z6Dyy+Ah1+fJl7NixA2vXrrX5udd+4YsJC7Nw9qga6UfUeH5ELlRqI7av9rZ5FmZiJmayTabf/uGF0J6lcA/So6JYwPlN7sj+VYWnv74GtZ/hrje4uQfp4Rmqv8vR6o/U7WQvmWrMxrOfHTp0CL179za9Hj9+PAAgNjYWK1aswMSJE1FcXIyRI0ciPz8fPXr0wNatW6FSqe51yLuSRSFfvnw5/P398cwzz9j83CkbvaDxMeC196/By0+PjJNu+HBoOPLzXB68MzMxEzPZZabSWwr8b6IfSm44w9XDCJ9IHZ7++hpCupfa5Pw1JXU72UsmuerVqxfE+xR/QRAwa9YszJo1y6rzCOL9zmIDRqMR4eHhGDJkCD7++GOL9tVqtdBoNOiFgXAW+EtE5MhGns2QOkI1X7RqLnUE2dOLFdiFDSgoKLD6cum9VNWKmP6z4OxiWW/3z/QVZdi/ZVq9Zq0NyXvkO3bsQGZmJoYPHy51FCIiasg4+1n96Nu3732HHoiIiOjeJC/kREREtlBXD4SRGxZyIiJyDEaxcrFmfxliISciIsfQQK+Ry+aBMERERGQ59siJiMghCLDyGnmdJalbLOREROQYbPxkN1vh0DoREZEdY4+ciIgcAr9+RkREZM941zoRERHJDXvkRETkEARRhGDFDWvW7FufWMiJ/kThI785lQ03b0kdoRonC+dLtgU5zjR29usuUkeoptXwQ1JHkI7x98Wa/WWIQ+tERER2jD1yIiJyCBxaJyIismcN9K51FnIiInIMfLIbERERyQ175ERE5BD4ZDciIiJ7xqF1IiIikhv2yImIyCEIxsrFmv3liIWciIgcA4fWiYiISG7YIyciIsfAB8I0XAOG5eGFUTfg7adHxik3LJnSBOlpamZiJou89MZlPNInFyHhJSgvc8Lpoxp8/a8WuHpJ2jYC5NVOANCuqxYvjMxBRLti+ARUYNZbLbE/WfoJa6RsJ6//5MAj9TZcc8pgdHVCWYQ7cl8IQUVQ5QQ1TkV6+GzIRqMTBXC+VQ6DhwuKOjXGzeeDYVTb9k+53H6faqqhPqLV4YfWez57GyOnZ2PVgkDE9WuFjFMqzEnKgMangpmYySLtuuRj8+omGD+0Mz4c2REKZyPmLE2D0s0gSZ4qcmsnAFCpjcg4rcaS6c0ky3AnqdtJnV6I/Mf9kTklClf+3gowiAhZcBaCrvL3xzm/As755ch9ORSXZ7fFtTeaodGJAgQsv2yTfFWkbieqTtJCbjAYMHXqVISHh8PNzQ0tWrTA7NmzIdrwU8+gkXnYmuSN7T94I/OcCosmhUBXKqDfEOmmjmQm+8w0bVQH7NgQhMwLjXDxrDsWTImCf7AOLdsUSpKnitzaCQAOpTTGNwtCsW+79L3wKlK309XxraDt4YvyJm4ob6rG9eHN4HKzHKpLJQCA8hA35MRFoLhjY1T4q1Aa5Ym8QU3Q6Gg+YHDsv5k1VnWzmzWLDElayOfNm4fExER89tlnOH36NObNm4f58+fj008/tcn5nV2MaBldgsN7PEzrRFHAkT0eaNO5xCYZmKnhZLpTI3c9AKCwQLorWPbQTnIgx3ZyKq3siRsa3fv3x6nUAKNKASgEm2SSYztZRMQfc5LXZpFnHZf2Gvm+ffswcOBAPPPMMwCAZs2a4fvvv8dvv/121+11Oh10Op3ptVarter8nt4GKJyB/FzzZrid54zQCN099qpfzGS/mf5MEES8Nek8Th7W4PJ5d8lyyL2d5EJ27WQU4fd9Fkoj3FEe4nbXTZwKK+CzKQcFPX1tFkt27WQhXiOvB4888gh27tyJs2fPAgCOHj2KvXv3on///nfdPiEhARqNxrSEhobaMi5Rjb3z4VmERRTj44ltpI5Cdsj/u0wor5Yi5+3md33fqdSAJgvPozxIhZsDg22cjuRG0h755MmTodVq0bp1aygUChgMBsyZMwdDhw696/bx8fEYP3686bVWq7WqmGtvKWDQA4399GbrvXz1uJ0rTdMwk/1mqjLqg7N4uOdNTBzWCTevqyTNIud2khM5tZP/d5fR6Gg+sia3ht7btdr7QqkBTRachVHlhOx3IwBn2/XH5NROtSLCygfC1FmSOiVpj3zNmjVYtWoVkpKScPjwYaxcuRL/+Mc/sHLlyrtur1Qq4enpabZYQ1/hhHPH1OjU44+bkQRBRMceRTiVKs1XKZjJfjMBIkZ9cBYxj+ci/o2OuH717kOitiTPdpIfWbSTKML/u8twP5yPKxMjofdTVtvEqdSAkAVnIToLyH4vAqKLbf+Ey6KdrNFAb3aT9CPU+++/j8mTJ+OVV14BALRv3x6XL19GQkICYmNjbZJh7Re+mLAwC2ePqpF+RI3nR+RCpTZi+2rp7qZlJvvM9M6HZ9Hr6RuYNaYdSosV8PKpvGZYXOSMcp1CkkyA/NoJAFRqA4LDykyvA0J1aB5VjMICZ+RmVy9gtiB1O/l/lwmPA7eQ/V4EjCoFFAWVX+cyuikgujpVDqf/8yycyo3IHtECTmVGoKzy4d8GD2fAyTY3vEndTlSdpIW8pKQETk7mnygVCgWMRts9mT5loxc0Pga89v41ePnpkXHSDR8ODUd+novNMjBTw8j011eyAQDzl6eZrV8wpTV2bAiSIFElubUTALRsX4z53582vX5rSiYAIPlHXyyY2EKSTFK3U+P/5QIAQuelm62/NrwZtD18obxcDLeMYgBA+OQTZttkzG8Pva9tPgBJ3U5WMQKw5vOOTCdNEURbfmn7DsOGDcOOHTuwdOlStG3bFkeOHMHIkSMxfPhwzJs374H7a7VaaDQa9MJAOAt28EtEsqfwkV+vwnBTft/PdVJJe+3/boxlZQ/eyMbOft1F6gjVtBp+SOoIZvRiBXZhAwoKCqy+XHovVbXiiXYT4ayo/QcevUGHnSfm1zjrjBkzMHPmTLN1kZGROHPmTK0z3I2kPfJPP/0UU6dOxTvvvIMbN24gODgYb731FqZNmyZlLCIiojrRtm1b7Nixw/Ta2bnuy66khdzDwwMLFy7EwoULpYxBRESOoI6mMb3zGSZKpRJK5d17+s7OzggMDKz9OWvA4Z+1TkREDqKO7loPDQ01e6ZJQkLCPU957tw5BAcHo3nz5hg6dCgyMzPr/J9lB1/8IyIiko+srCyza+T36o1369YNK1asQGRkJHJycjBz5kw8+uijOHHiBDw8PO66T22wkBMRkWOoo6H1mj7H5M9PKY2Ojka3bt0QFhaGNWvW4I033qh9jjuwkBMRkWOQ+OtnjRs3RqtWrXD+/HnrDnQHXiMnIiKHUDVpijWLNYqKinDhwgUEBdXtcyVYyImIiOrBhAkTkJKSgkuXLmHfvn14/vnnoVAoMGTIkDo9D4fWiYjIMdTRNfKaunLlCoYMGYKbN2/Cz88PPXr0wIEDB+Dn51f7DHfBQk5ERI7BKAKCFYXcaNm+q1evrv25LMChdSIiIjvGHjkRETkGGw+t2woLOREROQhr5xRnISeSPTnONCZHcpxpTI7kNtMYAGzLTpM6ghltoRFeraROYd9YyImIyDFwaJ2IiMiOGUVYNTxu4V3rtsK71omIiOwYe+REROQYRGPlYs3+MsRCTkREjoHXyImIiOwYr5ETERGR3LBHTkREjoFD60RERHZMhJWFvM6S1CkOrRMREdkx9siJiMgxcGidiIjIjhmNAKz4LriR3yOXrQHD8vDCqBvw9tMj45QblkxpgvQ0NTMxEzMxk0Nn2rTSB//5xhfXs1wBAGGRZRg67hq6Pl4IAMi+5IovZwXj5G/uqCgX0Lm3FnEfXYWXn94m+aiSw18j7/nsbYycno1VCwIR168VMk6pMCcpAxqfCmZiJmZiJofO5BdUgeEfZOOzren4dMtZdOheiBmvh+NSugplJU74YEgLCAIw79/nsWDDOejLnTAtNlyuHdc/htatWWRI0kJeWFiIsWPHIiwsDG5ubnjkkUdw8OBBm2YYNDIPW5O8sf0Hb2SeU2HRpBDoSgX0GyLddJbMxEzMxExyyPSXvlo8/EQhmjQvR0gLHV6ffA2qRkacSVXj5G+NcD3LFX9fmInwqDKER5Xh/U8u49xRNdL2utskn8VYyOvem2++ieTkZHz77bc4fvw4+vbtiz59+uDq1as2Ob+zixEto0tweI+HaZ0oCjiyxwNtOpfYJAMzMRMzMZM9ZDIYgF3rG0NX4oSoLsWoKBcAAXBx/aO4uShFCE7Ayd9kWsgbKMkKeWlpKX766SfMnz8fjz32GCIiIjBjxgxEREQgMTHxrvvodDpotVqzxRqe3gYonIH8XPNbBW7nOUt2jYeZmImZmElOmS6eVmFgRHv8tVkHLJocimnLLiKslQ6tOxdDpTZi2ZxglJUIKCtxwpezgmE0CLh1Q6a3XxlF6xcZkqyQ6/V6GAwGqFQqs/Vubm7Yu3fvXfdJSEiARqMxLaGhobaISkTksEJa6LAkOR2L/nMWf30tD/8YE4bLZ5Vo7GPAlKWX8GuyJ55rGY3nI9ujWKtARPsSCDK9+0oUjVYvciTZxyYPDw/ExMRg9uzZiIqKQkBAAL7//nvs378fERERd90nPj4e48ePN73WarVWFXPtLQUMeqDxHZ9uvXz1uJ0rTdMwEzMxEzPJKZOLq4gm4eUAgJbRpUhPU2P9V34YM/8KOvcqxIr9p1FwUwGFM+CuMeCVDm0R1FRns3wWEa3sVfMaeXXffvstRFFEkyZNoFQqsWjRIgwZMgROTnePpVQq4enpabZYQ1/hhHPH1OjUo9C0ThBEdOxRhFOp0nzlhJmYiZmYSa6ZgMpaVlFu/jda42OAu8aAtL3uyM9zxl/6WnfZkywj6YWMFi1aICUlBcXFxdBqtQgKCsLLL7+M5s2b2yzD2i98MWFhFs4eVSP9iBrPj8iFSm3E9tXeNsvATMzETMwkx0xfzw1C18e18GtSgdIiJ/xvnReO7XPHnKQLAIBtq73RtGUZND56nE5thMRpTfD8yFyERsi4R27NA9Nl2iOXxR0JjRo1QqNGjXD79m1s27YN8+fPt9m5UzZ6QeNjwGvvX4OXnx4ZJ93w4dBw5Oe52CwDMzETMzGTHDPl5znj/94Lw60bzlB7GBAeVYY5SRfQuWcRAODKBSWWJwShMF+BgNByDHnvOgaNzLVJtloxGgHBiuvcMr1GLoiidB8xtm3bBlEUERkZifPnz+P999+HSqXCnj174OLy4F9UrVYLjUaDXhgIZ0G6/9mIiOzFtuw0qSOY0RYa4dUqAwUFBVZfLr3nOX6vFU94DIWz4Frr4+jFcuwsXFWvWWtD0h55QUEB4uPjceXKFXh7e2Pw4MGYM2dOjYo4ERGRRTi0XvdeeuklvPTSS1JGICIiByEajRCtGFqX69fPZPptPyIiIqoJWdzsRkREVO84tE5ERGTHjCIgNLxCzqF1IiIiO8YeOREROQZRBGDN98jl2SNnISciIocgGkWIVgytS/jYlftiISciIscgGmFdj5xfPyMiInI4ixcvRrNmzaBSqdCtWzf89ttvdXp8FnIiInIIolG0erHUDz/8gPHjx2P69Ok4fPgwOnTogH79+uHGjRt19u9iISciIscgGq1fLLRgwQKMGDECr7/+Otq0aYPPP/8carUaX3/9dZ39s+z6GnnVjQd6VFj1HX8iIkehLZTXdV5tUWUeW9xIZm2t0KMCQOUkLH+mVCqhVCqrbV9eXo7U1FTEx8eb1jk5OaFPnz7Yv39/7YPcwa4LeWFhIQBgL/4rcRIiIvvg1UrqBHdXWFgIjUZTL8d2dXVFYGAg9l6zvla4u7sjNDTUbN306dMxY8aMatvm5eXBYDAgICDAbH1AQADOnDljdZYqdl3Ig4ODkZWVBQ8PDwiCYNWxtFotQkNDkZWVJZvp6ZipZuSWSW55AGaqKWaqmbrMJIoiCgsLERwcXEfpqlOpVLh48SLKy8utPpYoitXqzd1647Zk14XcyckJISEhdXpMT09P2fzPUoWZakZumeSWB2CmmmKmmqmrTPXVE/8zlUoFlUpV7+f5M19fXygUCly/ft1s/fXr1xEYGFhn5+HNbkRERPXA1dUVnTt3xs6dO03rjEYjdu7ciZiYmDo7j133yImIiORs/PjxiI2NRZcuXfDwww9j4cKFKC4uxuuvv15n52Ah/51SqcT06dMlv9bxZ8xUM3LLJLc8ADPVFDPVjBwzydXLL7+M3NxcTJs2DdeuXUPHjh2xdevWajfAWUMQ5frwWCIiInogXiMnIiKyYyzkREREdoyFnIiIyI6xkBMREdkxFnLU/xRzltq9ezcGDBiA4OBgCIKA9evXS5onISEBXbt2hYeHB/z9/fHcc88hPT1d0kyJiYmIjo42PZAiJiYGW7ZskTTTnT7++GMIgoCxY8dKlmHGjBkQBMFsad26tWR5qly9ehWvvvoqfHx84Obmhvbt2+PQoUOS5WnWrFm1dhIEAXFxcZJlMhgMmDp1KsLDw+Hm5oYWLVpg9uzZNnkm+f0UFhZi7NixCAsLg5ubGx555BEcPHhQ0kyOzuELuS2mmLNUcXExOnTogMWLF0uW4c9SUlIQFxeHAwcOIDk5GRUVFejbty+Ki4slyxQSEoKPP/4YqampOHToEB5//HEMHDgQJ0+elCzTnx08eBBLly5FdHS01FHQtm1b5OTkmJa9e/dKmuf27dvo3r07XFxcsGXLFpw6dQr//Oc/4eXlJVmmgwcPmrVRcnIyAODFF1+ULNO8efOQmJiIzz77DKdPn8a8efMwf/58fPrpp5JlAoA333wTycnJ+Pbbb3H8+HH07dsXffr0wdWrVyXN5dBEB/fwww+LcXFxptcGg0EMDg4WExISJEz1BwDiunXrpI5h5saNGyIAMSUlReooZry8vMSvvvpK6hhiYWGh2LJlSzE5OVns2bOnOGbMGMmyTJ8+XezQoYNk57+bSZMmiT169JA6xn2NGTNGbNGihWg0GiXL8Mwzz4jDhw83Wzdo0CBx6NChEiUSxZKSElGhUIibN282W//QQw+JH374oUSpyKF75FVTzPXp08e0rj6mmGtoCgoKAADe3t4SJ6lkMBiwevVqFBcX1+ljD2srLi4OzzzzjNnvlZTOnTuH4OBgNG/eHEOHDkVmZqakeTZu3IguXbrgxRdfhL+/Pzp16oQvv/xS0kx/Vl5eju+++w7Dhw+3ejImazzyyCPYuXMnzp49CwA4evQo9u7di/79+0uWSa/Xw2AwVHtmuZubm+QjPY7MoZ/sZqsp5hoSo9GIsWPHonv37mjXrp2kWY4fP46YmBiUlZXB3d0d69atQ5s2bSTNtHr1ahw+fFg21wy7deuGFStWIDIyEjk5OZg5cyYeffRRnDhxAh4eHpJkysjIQGJiIsaPH48PPvgABw8exHvvvQdXV1fExsZKkunP1q9fj/z8fAwbNkzSHJMnT4ZWq0Xr1q2hUChgMBgwZ84cDB06VLJMHh4eiImJwezZsxEVFYWAgAB8//332L9/PyIiIiTL5egcupCT5eLi4nDixAlZfPqOjIxEWloaCgoK8OOPPyI2NhYpKSmSFfOsrCyMGTMGycnJNp9l6V7+3HuLjo5Gt27dEBYWhjVr1uCNN96QJJPRaESXLl0wd+5cAECnTp1w4sQJfP7557Io5MuWLUP//v3rdVrNmlizZg1WrVqFpKQktG3bFmlpaRg7diyCg4Mlbadvv/0Ww4cPR5MmTaBQKPDQQw9hyJAhSE1NlSyTo3PoQm6rKeYaitGjR2Pz5s3YvXt3nU8fWxuurq6mXkDnzp1x8OBBfPLJJ1i6dKkkeVJTU3Hjxg089NBDpnUGgwG7d+/GZ599Bp1OB4VCIUm2Ko0bN0arVq1w/vx5yTIEBQVV+7AVFRWFn376SaJEf7h8+TJ27NiBtWvXSh0F77//PiZPnoxXXnkFANC+fXtcvnwZCQkJkhbyFi1aICUlBcXFxdBqtQgKCsLLL7+M5s2bS5bJ0Tn0NXJbTTFn70RRxOjRo7Fu3Tr8/PPPCA8PlzrSXRmNRuh0OsnO/8QTT+D48eNIS0szLV26dMHQoUORlpYmeREHgKKiIly4cAFBQUGSZejevXu1ry+ePXsWYWFhEiX6w/Lly+Hv749nnnlG6igoKSmBk5P5n2iFQgGj0ShRInONGjVCUFAQbt++jW3btmHgwIFSR3JYDt0jB2wzxZylioqKzHpMFy9eRFpaGry9vdG0aVOb54mLi0NSUhI2bNgADw8PXLt2DQCg0Wjg5uZm8zwAEB8fj/79+6Np06YoLCxEUlISdu3ahW3btkmSB6i8fnjnfQONGjWCj4+PZPcTTJgwAQMGDEBYWBiys7Mxffp0KBQKDBkyRJI8ADBu3Dg88sgjmDt3Ll566SX89ttv+OKLL/DFF19Ilgmo/CC4fPlyxMbGwtlZ+j+NAwYMwJw5c9C0aVO0bdsWR44cwYIFCzB8+HBJc23btg2iKCIyMhLnz5/H+++/j9atW0v6N9PhSX3bvBx8+umnYtOmTUVXV1fx4YcfFg8cOCBpnv/9738igGpLbGysJHnulgWAuHz5cknyiKIoDh8+XAwLCxNdXV1FPz8/8YknnhC3b98uWZ57kfrrZy+//LIYFBQkurq6ik2aNBFffvll8fz585LlqbJp0yaxXbt2olKpFFu3bi1+8cUXUkcSt23bJgIQ09PTpY4iiqIoarVaccyYMWLTpk1FlUolNm/eXPzwww9FnU4naa4ffvhBbN68uejq6ioGBgaKcXFxYn5+vqSZHB2nMSUiIrJjDn2NnIiIyN6xkBMREdkxFnIiIiI7xkJORERkx1jIiYiI7BgLORERkR1jISciIrJjLORERER2jIWcyErDhg3Dc889Z3rdq1cvjB071uY5du3aBUEQkJ+ff89tBEHA+vXra3zMGTNmoGPHjlblunTpEgRBQFpamlXHIaK7YyGnBmnYsGEQBAGCIJhmSZs1axb0en29n3vt2rWYPXt2jbatSfElIrof6WcGIKonTz31FJYvXw6dTof//ve/iIuLg4uLC+Lj46ttW15eDldX1zo5r7e3d50ch4ioJtgjpwZLqVQiMDAQYWFhGDVqFPr06YONGzcC+GM4fM6cOQgODkZkZCQAICsrCy+99BIaN24Mb29vDBw4EJcuXTId02AwYPz48WjcuDF8fHwwceJE3DldwZ1D6zqdDpMmTUJoaCiUSiUiIiKwbNkyXLp0Cb179wYAeHl5QRAEDBs2DEDlTFwJCQkIDw+Hm5sbOnTogB9//NHsPP/973/RqlUruLm5oXfv3mY5a2rSpElo1aoV1Go1mjdvjqlTp6KioqLadkuXLkVoaCjUajVeeuklFBQUmL3/1VdfISoqCiqVCq1bt8aSJUsszkJEtcNCTg7Dzc0N5eXlptc7d+5Eeno6kpOTsXnzZlRUVKBfv37w8PDAnj178Msvv8Dd3R1PPfWUab9//vOfWLFiBb7++mvs3bsXt27dwrp16+573tdeew3ff/89Fi1ahNOnT2Pp0qVwd3dHaGgofvrpJwBAeno6cnJy8MknnwAAEhIS8M033+Dzzz/HyZMnMW7cOLz66qtISUkBUPmBY9CgQRgwYADS0tLw5ptvYvLkyRa3iYeHB1asWIFTp07hk08+wZdffol//etfZtucP38ea9aswaZNm7B161YcOXIE77zzjun9VatWYdq0aZgzZw5Onz6NuXPnYurUqVi5cqXFeYioFiSefY2oXsTGxooDBw4URVEUjUajmJycLCqVSnHChAmm9wMCAsymhPz222/FyMhI0Wg0mtbpdDrRzc1N3LZtmyiKohgUFCTOnz/f9H5FRYUYEhJiOpcomk9dmp6eLgIQk5OT75qzasra27dvm9aVlZWJarVa3Ldvn9m2b7zxhjhkyBBRFEUxPj5ebNOmjdn7kyZNqnasOwEQ161bd8/3/+///k/s3Lmz6fX06dNFhUIhXrlyxbRuy5YtopOTk5iTkyOKoii2aNFCTEpKMjvO7NmzxZiYGFEURfHixYsiAPHIkSP3PC8R1R6vkVODtXnzZri7u6OiogJGoxF/+9vfMGPGDNP77du3N7sufvToUZw/fx4eHh5mxykrK8OFCxdQUFCAnJwcdOvWzfSes7MzunTpUm14vUpaWhoUCgV69uxZ49znz59HSUkJnnzySbP15eXl6NSpEwDg9OnTZjkAICYmpsbnqPLDDz9g0aJFuHDhAoqKiqDX6+Hp6Wm2TdOmTdGkSROz8xiNRqSnp8PDwwMXLlzAG2+8gREjRpi20ev10Gg0FuchIsuxkFOD1bt3byQmJsLV1RXBwcFwdjb/dW/UqJHZ66KiInTu3BmrVq2qdiw/P79aZXBzc7N4n6KiIgDAf/7zH7MCClRe968r+/fvx9ChQzFz5kz069cPGo0Gq1evxj//+U+Ls3755ZfVPlgoFIo6y0pE98ZCTg1Wo0aNEBERUePtH3roIfzwww/w9/ev1iutEhQUhF9//RWPPfYYgMqeZ2pqKh566KG7bt++fXsYjUakpKSgT58+1d6vGhEwGAymdW3atIFSqURmZuY9e/JRUVGmG/eqHDhw4MH/yD/Zt28fwsLC8OGHH5rWXb58udp2mZmZyM7ORnBwsOk8Tk5OiIyMREBAAIKDg5GRkYGhQ4dadH4iqhu82Y3od0OHDoWvry8GDhyIPXv24OLFi9i1axfee+89XLlyBQAwZswYfPzxx1i/fj3OnDmDd955577fAW/WrBliY2MxfPhwrF+/3nTMNWvWAADCwsIgCAI2b96M3NxcFBUVwcPDAxMmTMC4ceOwcuVKXLhwAYcPH8ann35quoHs7bffxrlz5/D+++8jPT0dSUlJWLFihUX/3pYtWyIzMxOrV6/GhQsXsGjRorveuKdSqRAbG4ujR49iz549eO+99/DSSy8hMDAQADBz5kwkJCRg0aJFOHv2LI4fP47ly5djwYIFFuUhotphISf6nVqtxu7du9G0aVMMGjQIUVFReOONN1BWVmbqof/973/H//t//w+xsbGIiYmBh4cHnn/++fseNzExES+88ALeeecdtG7dGiNGjEBxcTEAoEmTJpg5cyYmT56MgIAAjB49GgAwe/ZsTJ06FQkJCYiKisJTTz2F//znPwgPDwdQed36p59+wvr169GhQwd8/vnnmDt3rkX/3meffRbjxo3D6NGj0bFjR+zbtw9Tp06ttl1ERAQGDRqEp59+Gn379kV0dLTZ18vefPNNfPXVV1i+fDnat2+Pnj17YsWKFaasRFS/BPFed+kQERGR7LFHTkREZMdYyImIiOwYCzkREZEdYyEnIiKyYyzkREREdoyFnIiIyI6xkBMREdkxFnIiIiI7xkJORERkx1jIiYiI7BgLORERkR37/wFvgJiSrGOrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Test:  87.71428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "gH_uu1omYuqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "input = data[:, 1:8]\n",
        "output = data[:, 0]\n",
        "output = output - 1\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=0.1, random_state=42)\n",
        "\n",
        "X_labeled, X_unlabeled, y_labeled, _ = train_test_split(X_train, y_train, test_size=0.9, random_state=42)\n",
        "\n",
        "\n",
        "pca = PCA(n_components=7)\n",
        "pca.fit(X_train)\n",
        "\n",
        "X_train_pca = pca.transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "X_labeled_pca = pca.transform(X_labeled)\n",
        "X_unlabeled_pca = pca.transform(X_unlabeled)\n",
        "\n",
        "minmax = MinMaxScaler()\n",
        "minmax.fit(X_train_pca)\n",
        "\n",
        "X_train_scaled = minmax.transform(X_train_pca).astype(np.float32)\n",
        "X_test_scaled = minmax.transform(X_test_pca).astype(np.float32)\n",
        "X_labeled_scaled = minmax.transform(X_labeled_pca).astype(np.float32)\n",
        "X_unlabeled_scaled = minmax.transform(X_unlabeled_pca).astype(np.float32)\n",
        "\n",
        "\n",
        "num_classes = len(np.unique(y_labeled))\n",
        "y_labeled_onehot = tf.keras.utils.to_categorical(y_labeled, num_classes=num_classes)\n",
        "labels = y_labeled_onehot\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "\n",
        "labeled_dataset = tf.data.Dataset.from_tensor_slices((X_labeled_scaled, y_labeled_onehot))\n",
        "labeled_dataset = labeled_dataset.shuffle(buffer_size=len(X_labeled_scaled)).batch(batch_size)\n",
        "\n",
        "\n",
        "unlabeled_dataset = tf.data.Dataset.from_tensor_slices(X_unlabeled_scaled)\n",
        "unlabeled_dataset = unlabeled_dataset.shuffle(buffer_size=len(X_unlabeled_scaled)).batch(batch_size)\n",
        "\n",
        "num_classes = len(np.unique(y_test))\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_scaled, y_test_onehot))\n",
        "test_dataset = test_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "NK0rKWOVYuMl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}